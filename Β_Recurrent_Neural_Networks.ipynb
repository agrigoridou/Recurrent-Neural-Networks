{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgIm61ToaIQlRQwlsfmt/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrigoridou/Recurrent-Neural-Networks/blob/main/%CE%92_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Εγκατάσταση Απαιτούμενων Βιβλιοθηκών"
      ],
      "metadata": {
        "id": "GrIsLAs_v-RR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pxx07rpt-xd",
        "outputId": "2800cc15-a211-472e-c31a-c2acd13b652f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Collecting torch==1.13.0 (from torchtext)\n",
            "  Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n",
            "Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.0\n",
        "!pip install torchtext==0.14.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hES_geOSp3zo",
        "outputId": "2418ec8a-0fb6-4dac-94bd-c1abc158b3b4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.45.1)\n",
            "Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m838.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "72c63d752d1d4518a21cdb02b0f8c3ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.14.0\n",
            "  Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (2.32.3)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2024.12.14)\n",
            "Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "Successfully installed torchtext-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecl3NueoNBcT",
        "outputId": "c4d912a9-4456-4068-98f1-24b13a7b935d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "_umWPsLtoHd4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "hWSBZ_32oFtJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "TDgaukMONKj2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtPssc0vDSy",
        "outputId": "7dad1236-9d79-4c47-93f1-7bed90db7e88"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update paths in the RNN code\n",
        "updated_rnn_code = rnn_code.replace('ag-news-classification-dataset/train.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
        "updated_rnn_code = updated_rnn_code.replace('ag-news-classification-dataset/test.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
        "\n",
        "# Save the updated file to the correct location\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNNkvga5u9n7",
        "outputId": "3c597fe2-163e-4cff-dbb2-9e6c7415b92e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Εκτέλεση του κώδικα"
      ],
      "metadata": {
        "id": "qADmQnx10zNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472ccfbf-a668-4227-8f6b-b8d54133aea7",
        "id": "-U2FoFbhufTk"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:17<00:00,  6.36it/s]\n",
            "Train Loss : 1.301\n",
            "Valid Loss : 1.174 Valid Acc  : 0.563\n",
            "Epoch: 2\n",
            "100% 112/112 [00:17<00:00,  6.58it/s]\n",
            "Train Loss : 1.084\n",
            "Valid Loss : 1.023 Valid Acc  : 0.721\n",
            "Epoch: 3\n",
            "100% 112/112 [00:16<00:00,  6.81it/s]\n",
            "Train Loss : 0.978\n",
            "Valid Loss : 0.959 Valid Acc  : 0.785\n",
            "Epoch: 4\n",
            "100% 112/112 [00:16<00:00,  6.77it/s]\n",
            "Train Loss : 0.932\n",
            "Valid Loss : 0.926 Valid Acc  : 0.819\n",
            "Epoch: 5\n",
            "100% 112/112 [00:16<00:00,  6.79it/s]\n",
            "Train Loss : 0.905\n",
            "Valid Loss : 0.911 Valid Acc  : 0.832\n",
            "Epoch: 6\n",
            "100% 112/112 [00:16<00:00,  6.73it/s]\n",
            "Train Loss : 0.888\n",
            "Valid Loss : 0.901 Valid Acc  : 0.842\n",
            "Epoch: 7\n",
            "100% 112/112 [00:16<00:00,  6.66it/s]\n",
            "Train Loss : 0.875\n",
            "Valid Loss : 0.896 Valid Acc  : 0.845\n",
            "Epoch: 8\n",
            "100% 112/112 [00:17<00:00,  6.32it/s]\n",
            "Train Loss : 0.866\n",
            "Valid Loss : 0.892 Valid Acc  : 0.850\n",
            "Epoch: 9\n",
            "100% 112/112 [00:17<00:00,  6.53it/s]\n",
            "Train Loss : 0.860\n",
            "Valid Loss : 0.889 Valid Acc  : 0.855\n",
            "Epoch: 10\n",
            "100% 112/112 [00:16<00:00,  6.69it/s]\n",
            "Train Loss : 0.853\n",
            "Valid Loss : 0.882 Valid Acc  : 0.860\n",
            "Epoch: 11\n",
            "100% 112/112 [00:16<00:00,  6.75it/s]\n",
            "Train Loss : 0.849\n",
            "Valid Loss : 0.885 Valid Acc  : 0.858\n",
            "Epoch: 12\n",
            "100% 112/112 [00:16<00:00,  6.65it/s]\n",
            "Train Loss : 0.844\n",
            "Valid Loss : 0.879 Valid Acc  : 0.864\n",
            "Epoch: 13\n",
            "100% 112/112 [00:16<00:00,  6.77it/s]\n",
            "Train Loss : 0.840\n",
            "Valid Loss : 0.878 Valid Acc  : 0.864\n",
            "Epoch: 14\n",
            "100% 112/112 [00:17<00:00,  6.48it/s]\n",
            "Train Loss : 0.835\n",
            "Valid Loss : 0.878 Valid Acc  : 0.863\n",
            "Epoch: 15\n",
            "100% 112/112 [00:17<00:00,  6.42it/s]\n",
            "Train Loss : 0.832\n",
            "Valid Loss : 0.877 Valid Acc  : 0.866\n",
            "\n",
            "Test Accuracy : 0.862\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.90      0.86      0.88      1900\n",
            "      Sports       0.92      0.93      0.92      1900\n",
            "    Business       0.85      0.79      0.82      1900\n",
            "    Sci/Tech       0.79      0.87      0.83      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1630   72  108   90]\n",
            " [  44 1758   31   67]\n",
            " [  75   35 1505  285]\n",
            " [  71   40  130 1659]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια στο Test Set: 86.2%\n",
        "(ii) Ακρίβεια στο Validation Set: 86.6%\n",
        "(iii) Πλήθος Παραμέτρων: 2136284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης ανά Εποχή: περίπου 16-17 δευτερόλεπτα ανά εποχή.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kDRQv80u3V0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Δι-κατευθυντήριο RNN"
      ],
      "metadata": {
        "id": "tWyIvMXA0vr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δι-κατευθυντήριο RNN: Στην τρέχουσα εκδοχή του κώδικα, το RNN layer είναι μονοκατευθυντικό (unidirectional). Για να το κάνουμε δι-κατευθυντήριο, πρέπει να προσθέσουμε το bidirectional=True στον ορισμό του RNN layer. Επίσης, επειδή το δι-κατευθυντήριο RNN συνενώνει τις εξόδους των δύο κατευθύνσεων, το μέγεθος του διανύσματος εξόδου θα διπλασιαστεί, οπότε θα πρέπει να αυξήσουμε το μέγεθος της εξόδου του γραμμικού (linear) layer για να το αντιστοιχίσουμε με το νέο μέγεθος του διανύσματος εξόδου."
      ],
      "metadata": {
        "id": "LlA5CZU74Eik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αλλαγές στον κώδικα:\n",
        "\n",
        "*   Τροποποιούμε την κλάση του RNN για να είναι δι-κατευθυντική.\n",
        "*   Διπλασιάζουμε το μέγεθος των εξόδων του RNN στον γραμμικό (linear) layer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3XpBfBFf4cFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "HzgBl1Zr09RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a819ae-ea4d-4777-9bdc-1861f30a6ff5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικαθιστούμε το RNN με bidirectional RNN και προσαρμόζουμε το γραμμικό layer\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'  # Για διπλασιασμένο hidden_dim λόγω bidirectional\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "KgWU52HE1A6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eaaf0e-ff80-4f6e-f607-8ca0daa815db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXpDw-Hu6isI",
        "outputId": "48fa4382-9af2-49c7-cf42-3aa692093553"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:25<00:00,  4.48it/s]\n",
            "Train Loss : 1.310\n",
            "Valid Loss : 1.177 Valid Acc  : 0.571\n",
            "Epoch: 2\n",
            "100% 112/112 [00:23<00:00,  4.71it/s]\n",
            "Train Loss : 1.068\n",
            "Valid Loss : 1.001 Valid Acc  : 0.742\n",
            "Epoch: 3\n",
            "100% 112/112 [00:25<00:00,  4.42it/s]\n",
            "Train Loss : 0.965\n",
            "Valid Loss : 0.946 Valid Acc  : 0.798\n",
            "Epoch: 4\n",
            "100% 112/112 [00:25<00:00,  4.41it/s]\n",
            "Train Loss : 0.922\n",
            "Valid Loss : 0.926 Valid Acc  : 0.818\n",
            "Epoch: 5\n",
            "100% 112/112 [00:24<00:00,  4.67it/s]\n",
            "Train Loss : 0.900\n",
            "Valid Loss : 0.911 Valid Acc  : 0.833\n",
            "Epoch: 6\n",
            "100% 112/112 [00:25<00:00,  4.39it/s]\n",
            "Train Loss : 0.885\n",
            "Valid Loss : 0.901 Valid Acc  : 0.842\n",
            "Epoch: 7\n",
            "100% 112/112 [00:25<00:00,  4.39it/s]\n",
            "Train Loss : 0.874\n",
            "Valid Loss : 0.894 Valid Acc  : 0.848\n",
            "Epoch: 8\n",
            "100% 112/112 [00:24<00:00,  4.63it/s]\n",
            "Train Loss : 0.867\n",
            "Valid Loss : 0.890 Valid Acc  : 0.853\n",
            "Epoch: 9\n",
            "100% 112/112 [00:25<00:00,  4.43it/s]\n",
            "Train Loss : 0.859\n",
            "Valid Loss : 0.888 Valid Acc  : 0.855\n",
            "Epoch: 10\n",
            "100% 112/112 [00:26<00:00,  4.26it/s]\n",
            "Train Loss : 0.854\n",
            "Valid Loss : 0.884 Valid Acc  : 0.857\n",
            "Epoch: 11\n",
            "100% 112/112 [00:26<00:00,  4.26it/s]\n",
            "Train Loss : 0.851\n",
            "Valid Loss : 0.879 Valid Acc  : 0.862\n",
            "Epoch: 12\n",
            "100% 112/112 [00:23<00:00,  4.68it/s]\n",
            "Train Loss : 0.846\n",
            "Valid Loss : 0.878 Valid Acc  : 0.864\n",
            "Epoch: 13\n",
            "100% 112/112 [00:25<00:00,  4.45it/s]\n",
            "Train Loss : 0.842\n",
            "Valid Loss : 0.879 Valid Acc  : 0.864\n",
            "Epoch: 14\n",
            "100% 112/112 [00:25<00:00,  4.41it/s]\n",
            "Train Loss : 0.840\n",
            "Valid Loss : 0.877 Valid Acc  : 0.864\n",
            "Epoch: 15\n",
            "100% 112/112 [00:23<00:00,  4.70it/s]\n",
            "Train Loss : 0.837\n",
            "Valid Loss : 0.876 Valid Acc  : 0.866\n",
            "\n",
            "Test Accuracy : 0.863\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.88      0.86      0.87      1900\n",
            "      Sports       0.90      0.95      0.92      1900\n",
            "    Business       0.82      0.82      0.82      1900\n",
            "    Sci/Tech       0.85      0.82      0.83      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1635   93  109   63]\n",
            " [  33 1797   47   23]\n",
            " [  82   59 1565  194]\n",
            " [ 101   49  189 1561]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 86.3%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 84.2%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2147164\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 25 δευτερόλεπτα\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8a_ewFOF8uug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Διπλό στρώμα δι-κατευθυντήριου RNN"
      ],
      "metadata": {
        "id": "6U8igk7r1C36"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VuQorBTt1EL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Χρήση LSTM"
      ],
      "metadata": {
        "id": "B4VaUn8_1Fm-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLl4K2Lh1G6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}