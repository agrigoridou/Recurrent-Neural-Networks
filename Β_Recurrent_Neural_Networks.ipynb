{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTtZDMMa7U2iAG22/9kB3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrigoridou/Recurrent-Neural-Networks/blob/main/%CE%92_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Εγκατάσταση Απαιτούμενων Βιβλιοθηκών"
      ],
      "metadata": {
        "id": "GrIsLAs_v-RR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pxx07rpt-xd",
        "outputId": "b1857a47-5835-48e7-e918-9237ff84a461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecl3NueoNBcT",
        "outputId": "d1733014-96f8-46fd-d502-7da785089655"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "hWSBZ_32oFtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_umWPsLtoHd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "TDgaukMONKj2"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ελέγξτε τα αρχεία στον Google Drive και επιβεβαιώστε τις διαδρομές\n",
        "!ls /content/drive/MyDrive/RecurrentNeuralNetworks/\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP0TSjeSVqbY",
        "outputId": "f94bb1f1-ed15-48fb-b8e5-521aade22e1b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "rnn.py\ttest.csv.zip  train.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/RecurrentNeuralNetworks/train.csv.zip\" /content\n",
        "!cp \"/content/drive/MyDrive/RecurrentNeuralNetworks/test.csv.zip\" /content\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zx1_eXkocfLu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Στη συνέχεια, αποσυμπιέστε τα αρχεία\n",
        "!unzip -q /content/train.csv.zip -d /content\n",
        "!unzip -q /content/test.csv.zip -d /content"
      ],
      "metadata": {
        "id": "YcDCqK6abqMp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n"
      ],
      "metadata": {
        "id": "X5uzL5r_dDYh"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Εμφάνιση των πρώτων γραμμών των δεδομένων για επιβεβαίωση\n",
        "print(train_data.head())\n",
        "print(test_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqW8fJXZdT67",
        "outputId": "ae83d3e5-cd79-4e67-c20b-0e493a4e9aa2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Class Index                                              Title  \\\n",
            "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
            "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
            "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
            "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
            "4            3  Oil prices soar to all-time record, posing new...   \n",
            "\n",
            "                                         Description  \n",
            "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
            "1  Reuters - Private investment firm Carlyle Grou...  \n",
            "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
            "3  Reuters - Authorities have halted oil export\\f...  \n",
            "4  AFP - Tearaway world oil prices, toppling reco...  \n",
            "   Class Index                                              Title  \\\n",
            "0            3                  Fears for T N pension after talks   \n",
            "1            4  The Race is On: Second Private Team Sets Launc...   \n",
            "2            4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
            "3            4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
            "4            4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
            "\n",
            "                                         Description  \n",
            "0  Unions representing workers at Turner   Newall...  \n",
            "1  SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
            "2  AP - A company founded by a chemistry research...  \n",
            "3  AP - It's barely dawn when Mike Fitzpatrick st...  \n",
            "4  AP - Southern California's smog-fighting agenc...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\" /content\n"
      ],
      "metadata": {
        "id": "DX_Q_Uy5gpJt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "print(\"Code in rnn.py:\")\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFPKmam6dHSU",
        "outputId": "a0bd6a75-a56d-4edc-ac7d-fb756135bddd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code in rnn.py:\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n"
      ],
      "metadata": {
        "id": "TsX4mplBel99"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "print(\"Code in rnn.py:\")\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzxHj4uAj0mF",
        "outputId": "f52680cc-cb55-4beb-8ccd-c37b6ca21de2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code in rnn.py:\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "id": "87dxGGLmNvpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6882a7-e537-4a6f-cdf4-d0e34520c9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 14, in <module>\n",
            "    from torchtext.data import get_tokenizer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchtext/__init__.py\", line 18, in <module>\n",
            "    from torchtext import _extension  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\", line 64, in <module>\n",
            "    _init_extension()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\", line 58, in _init_extension\n",
            "    _load_lib(\"libtorchtext\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\", line 50, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1350, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: /usr/local/lib/python3.10/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Εκτέλεση του κώδικα"
      ],
      "metadata": {
        "id": "qADmQnx10zNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Για την ακρίβεια στο test set\n",
        "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "\n",
        "# Για την αναφορά ταξινόμησης\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "\n",
        "# Για τον πίνακα σύγχυσης\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))\n",
        "\n",
        "print('Total parameters: ', count_parameters(classifier))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QxszZq6j0wdq",
        "outputId": "f458b2e7-ae7a-430d-e541-f8f2c69bca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b7c091c294a6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Για την ακρίβεια στο test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTest Accuracy : {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Για την αναφορά ταξινόμησης\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClassification Report : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Δι-κατευθυντήριο RNN"
      ],
      "metadata": {
        "id": "tWyIvMXA0vr2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzgBl1Zr09RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgWU52HE1A6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Διπλό στρώμα δι-κατευθυντήριου RNN"
      ],
      "metadata": {
        "id": "6U8igk7r1C36"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VuQorBTt1EL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Χρήση LSTM"
      ],
      "metadata": {
        "id": "B4VaUn8_1Fm-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLl4K2Lh1G6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}