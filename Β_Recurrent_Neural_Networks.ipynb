{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXHC2GeOQX7v/5wRmVeSsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrigoridou/Recurrent-Neural-Networks/blob/main/%CE%92_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Εγκατάσταση Απαιτούμενων Βιβλιοθηκών"
      ],
      "metadata": {
        "id": "GrIsLAs_v-RR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pxx07rpt-xd",
        "outputId": "5eef2d1d-cb03-4c90-f608-013c2f4da754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.5.1\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Collecting torch==1.13.0 (from torchtext)\n",
            "  Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n",
            "Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecl3NueoNBcT",
        "outputId": "2f2f27f4-03a1-4466-f996-0153795de3fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "_umWPsLtoHd4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "hWSBZ_32oFtJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "TDgaukMONKj2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtPssc0vDSy",
        "outputId": "048cc915-43d0-4e6d-a28f-56d9c9e39b01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update paths in the RNN code\n",
        "updated_rnn_code = rnn_code.replace('ag-news-classification-dataset/train.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
        "updated_rnn_code = updated_rnn_code.replace('ag-news-classification-dataset/test.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
        "\n",
        "# Save the updated file to the correct location\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNNkvga5u9n7",
        "outputId": "a580e964-7632-45f3-d8fa-8143f76d2e74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Εκτέλεση του κώδικα"
      ],
      "metadata": {
        "id": "qADmQnx10zNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfcd6e24-19fb-412d-c110-bc06d183d019",
        "id": "-U2FoFbhufTk"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:15<00:00,  7.04it/s]\n",
            "Train Loss : 1.302\n",
            "Valid Loss : 1.162 Valid Acc  : 0.566\n",
            "Epoch: 2\n",
            "100% 112/112 [00:16<00:00,  6.98it/s]\n",
            "Train Loss : 1.082\n",
            "Valid Loss : 1.022 Valid Acc  : 0.720\n",
            "Epoch: 3\n",
            "100% 112/112 [00:15<00:00,  7.00it/s]\n",
            "Train Loss : 0.975\n",
            "Valid Loss : 0.962 Valid Acc  : 0.779\n",
            "Epoch: 4\n",
            "100% 112/112 [00:16<00:00,  6.86it/s]\n",
            "Train Loss : 0.931\n",
            "Valid Loss : 0.936 Valid Acc  : 0.804\n",
            "Epoch: 5\n",
            "100% 112/112 [00:28<00:00,  3.91it/s]\n",
            "Train Loss : 0.907\n",
            "Valid Loss : 0.918 Valid Acc  : 0.825\n",
            "Epoch: 6\n",
            "100% 112/112 [00:16<00:00,  6.81it/s]\n",
            "Train Loss : 0.888\n",
            "Valid Loss : 0.909 Valid Acc  : 0.831\n",
            "Epoch: 7\n",
            "100% 112/112 [00:16<00:00,  6.88it/s]\n",
            "Train Loss : 0.876\n",
            "Valid Loss : 0.894 Valid Acc  : 0.848\n",
            "Epoch: 8\n",
            "100% 112/112 [00:16<00:00,  6.91it/s]\n",
            "Train Loss : 0.866\n",
            "Valid Loss : 0.887 Valid Acc  : 0.854\n",
            "Epoch: 9\n",
            "100% 112/112 [00:16<00:00,  6.92it/s]\n",
            "Train Loss : 0.861\n",
            "Valid Loss : 0.884 Valid Acc  : 0.859\n",
            "Epoch: 10\n",
            "100% 112/112 [00:16<00:00,  6.87it/s]\n",
            "Train Loss : 0.853\n",
            "Valid Loss : 0.887 Valid Acc  : 0.855\n",
            "Epoch: 11\n",
            "100% 112/112 [00:16<00:00,  6.94it/s]\n",
            "Train Loss : 0.847\n",
            "Valid Loss : 0.878 Valid Acc  : 0.865\n",
            "Epoch: 12\n",
            "100% 112/112 [00:16<00:00,  6.78it/s]\n",
            "Train Loss : 0.842\n",
            "Valid Loss : 0.874 Valid Acc  : 0.869\n",
            "Epoch: 13\n",
            "100% 112/112 [00:16<00:00,  6.71it/s]\n",
            "Train Loss : 0.837\n",
            "Valid Loss : 0.874 Valid Acc  : 0.867\n",
            "Epoch: 14\n",
            "100% 112/112 [00:16<00:00,  6.60it/s]\n",
            "Train Loss : 0.834\n",
            "Valid Loss : 0.873 Valid Acc  : 0.870\n",
            "Epoch: 15\n",
            "100% 112/112 [00:16<00:00,  6.87it/s]\n",
            "Train Loss : 0.831\n",
            "Valid Loss : 0.873 Valid Acc  : 0.868\n",
            "\n",
            "Test Accuracy : 0.865\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.86      0.87      1900\n",
            "      Sports       0.93      0.94      0.94      1900\n",
            "    Business       0.80      0.85      0.82      1900\n",
            "    Sci/Tech       0.85      0.81      0.83      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.87      0.86      0.86      7600\n",
            "weighted avg       0.87      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1636   73  115   76]\n",
            " [  43 1794   40   23]\n",
            " [  79   34 1611  176]\n",
            " [  89   33  245 1533]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια στο Test Set: 86.5%\n",
        "(ii) Ακρίβεια στο Validation Set: 87.0%\n",
        "(iii) Πλήθος Παραμέτρων: 2136284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης ανά Εποχή: περίπου 16 δευτερόλεπτα ανά εποχή.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kDRQv80u3V0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Δι-κατευθυντήριο RNN"
      ],
      "metadata": {
        "id": "tWyIvMXA0vr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δι-κατευθυντήριο RNN: Στην τρέχουσα εκδοχή του κώδικα, το RNN layer είναι μονοκατευθυντικό (unidirectional). Για να το κάνουμε δι-κατευθυντήριο, πρέπει να προσθέσουμε το bidirectional=True στον ορισμό του RNN layer. Επίσης, επειδή το δι-κατευθυντήριο RNN συνενώνει τις εξόδους των δύο κατευθύνσεων, το μέγεθος του διανύσματος εξόδου θα διπλασιαστεί, οπότε θα πρέπει να αυξήσουμε το μέγεθος της εξόδου του γραμμικού (linear) layer για να το αντιστοιχίσουμε με το νέο μέγεθος του διανύσματος εξόδου."
      ],
      "metadata": {
        "id": "LlA5CZU74Eik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αλλαγές στον κώδικα:\n",
        "\n",
        "*   Τροποποιούμε την κλάση του RNN για να είναι δι-κατευθυντική.\n",
        "*   Διπλασιάζουμε το μέγεθος των εξόδων του RNN στον γραμμικό (linear) layer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3XpBfBFf4cFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "HzgBl1Zr09RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ac2336-f1f4-4e82-a900-1af79c85ee1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικαθιστούμε το RNN με bidirectional RNN και προσαρμόζουμε το γραμμικό layer\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'  # Για διπλασιασμένο hidden_dim λόγω bidirectional\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "KgWU52HE1A6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425bd0f3-3009-4d1f-ad7d-c19881738bca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXpDw-Hu6isI",
        "outputId": "f4d0ccff-1178-4ca1-993c-9f9486c8d958"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:23<00:00,  4.78it/s]\n",
            "Train Loss : 1.302\n",
            "Valid Loss : 1.161 Valid Acc  : 0.586\n",
            "Epoch: 2\n",
            "100% 112/112 [00:24<00:00,  4.66it/s]\n",
            "Train Loss : 1.076\n",
            "Valid Loss : 1.015 Valid Acc  : 0.731\n",
            "Epoch: 3\n",
            "100% 112/112 [00:24<00:00,  4.51it/s]\n",
            "Train Loss : 0.976\n",
            "Valid Loss : 0.963 Valid Acc  : 0.782\n",
            "Epoch: 4\n",
            "100% 112/112 [00:23<00:00,  4.82it/s]\n",
            "Train Loss : 0.933\n",
            "Valid Loss : 0.937 Valid Acc  : 0.807\n",
            "Epoch: 5\n",
            "100% 112/112 [00:24<00:00,  4.54it/s]\n",
            "Train Loss : 0.907\n",
            "Valid Loss : 0.922 Valid Acc  : 0.819\n",
            "Epoch: 6\n",
            "100% 112/112 [00:24<00:00,  4.51it/s]\n",
            "Train Loss : 0.891\n",
            "Valid Loss : 0.913 Valid Acc  : 0.829\n",
            "Epoch: 7\n",
            "100% 112/112 [00:23<00:00,  4.79it/s]\n",
            "Train Loss : 0.880\n",
            "Valid Loss : 0.897 Valid Acc  : 0.845\n",
            "Epoch: 8\n",
            "100% 112/112 [00:24<00:00,  4.50it/s]\n",
            "Train Loss : 0.870\n",
            "Valid Loss : 0.894 Valid Acc  : 0.848\n",
            "Epoch: 9\n",
            "100% 112/112 [00:24<00:00,  4.61it/s]\n",
            "Train Loss : 0.862\n",
            "Valid Loss : 0.892 Valid Acc  : 0.850\n",
            "Epoch: 10\n",
            "100% 112/112 [00:23<00:00,  4.84it/s]\n",
            "Train Loss : 0.857\n",
            "Valid Loss : 0.885 Valid Acc  : 0.857\n",
            "Epoch: 11\n",
            "100% 112/112 [00:25<00:00,  4.46it/s]\n",
            "Train Loss : 0.851\n",
            "Valid Loss : 0.883 Valid Acc  : 0.859\n",
            "Epoch: 12\n",
            "100% 112/112 [00:24<00:00,  4.56it/s]\n",
            "Train Loss : 0.848\n",
            "Valid Loss : 0.883 Valid Acc  : 0.858\n",
            "Epoch: 13\n",
            "100% 112/112 [00:30<00:00,  3.68it/s]\n",
            "Train Loss : 0.845\n",
            "Valid Loss : 0.881 Valid Acc  : 0.861\n",
            "Epoch: 14\n",
            "100% 112/112 [00:23<00:00,  4.74it/s]\n",
            "Train Loss : 0.840\n",
            "Valid Loss : 0.878 Valid Acc  : 0.866\n",
            "Epoch: 15\n",
            "100% 112/112 [00:24<00:00,  4.49it/s]\n",
            "Train Loss : 0.837\n",
            "Valid Loss : 0.882 Valid Acc  : 0.859\n",
            "\n",
            "Test Accuracy : 0.853\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.84      0.86      1900\n",
            "      Sports       0.92      0.93      0.93      1900\n",
            "    Business       0.82      0.79      0.81      1900\n",
            "    Sci/Tech       0.78      0.86      0.82      1900\n",
            "\n",
            "    accuracy                           0.85      7600\n",
            "   macro avg       0.86      0.85      0.85      7600\n",
            "weighted avg       0.86      0.85      0.85      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1587   85   94  134]\n",
            " [  42 1760   77   21]\n",
            " [  53   38 1501  308]\n",
            " [  94   20  148 1638]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 85.3%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 86.6%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2147164\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 25 δευτερόλεπτα\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8a_ewFOF8uug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Διπλό στρώμα δι-κατευθυντήριου RNN"
      ],
      "metadata": {
        "id": "6U8igk7r1C36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Αλλαγή στην αρχικοποίηση του RNN:\n",
        "\n",
        "\n",
        "*   Αυξάνουμε τον αριθμό των επιπέδων του RNN από 1 σε 2. Αυτό γίνεται με την παράμετρο num_layers=2.\n",
        "*   Ρυθμίζουμε το RNN ώστε να είναι δι-κατευθυντήριο με την παράμετρο bidirectional=True.\n",
        "\n",
        "\n",
        "\n",
        "##2. Αλλαγή στον γραμμικό (linear) layer:\n",
        "\n",
        "Επειδή το δι-κατευθυντήριο RNN διπλασιάζει το μέγεθος του hidden state, και\n",
        "\n",
        "*   τώρα έχουμε δύο στρώματα, το μέγεθος του hidden state θα είναι 2 * hidden_dim (διπλό λόγω δι-κατευθυντηριότητας και λόγω δύο στρωμάτων). Άρα το hidden_dim πρέπει να πολλαπλασιαστεί με 2 ακόμα, δηλαδή το νέο μέγεθος του hidden state είναι hidden_dim * 2 * 2 λόγω δύο επιπέδων και δι-κατευθυντηριότητας.\n",
        "\n"
      ],
      "metadata": {
        "id": "xdpR_QNQ_Wpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημερώνουμε το RNN για να είναι δύο στρώματα δι-κατευθυντήριου\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημερώνουμε το γραμμικό layer για να έχει το σωστό μέγεθος εξόδου\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'logits = self.linear(output[:, -1])',\n",
        "    'logits = self.linear(output[:, -1, :])'  # Για δύο στρώματα δι-κατευθυντήριου RNN, το μέγεθος είναι hidden_dim * 2 * 2\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "VuQorBTt1EL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36add6cc-6d8b-49d4-f1f1-1428a9b968f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvrZXvRe_PAY",
        "outputId": "f621ec5d-7018-4216-c31e-656db9fa277c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2171996\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:38<00:00,  2.92it/s]\n",
            "Train Loss : 1.255\n",
            "Valid Loss : 1.103 Valid Acc  : 0.634\n",
            "Epoch: 2\n",
            "100% 112/112 [00:38<00:00,  2.89it/s]\n",
            "Train Loss : 1.039\n",
            "Valid Loss : 0.996 Valid Acc  : 0.744\n",
            "Epoch: 3\n",
            "100% 112/112 [00:37<00:00,  3.02it/s]\n",
            "Train Loss : 0.970\n",
            "Valid Loss : 0.963 Valid Acc  : 0.776\n",
            "Epoch: 4\n",
            "100% 112/112 [00:38<00:00,  2.90it/s]\n",
            "Train Loss : 0.937\n",
            "Valid Loss : 0.939 Valid Acc  : 0.804\n",
            "Epoch: 5\n",
            "100% 112/112 [00:37<00:00,  3.00it/s]\n",
            "Train Loss : 0.916\n",
            "Valid Loss : 0.925 Valid Acc  : 0.816\n",
            "Epoch: 6\n",
            "100% 112/112 [00:38<00:00,  2.94it/s]\n",
            "Train Loss : 0.905\n",
            "Valid Loss : 0.945 Valid Acc  : 0.796\n",
            "Epoch: 7\n",
            "100% 112/112 [00:38<00:00,  2.88it/s]\n",
            "Train Loss : 0.897\n",
            "Valid Loss : 0.926 Valid Acc  : 0.814\n",
            "Epoch: 8\n",
            "100% 112/112 [00:37<00:00,  3.00it/s]\n",
            "Train Loss : 0.888\n",
            "Valid Loss : 0.908 Valid Acc  : 0.833\n",
            "Epoch: 9\n",
            "100% 112/112 [00:38<00:00,  2.90it/s]\n",
            "Train Loss : 0.877\n",
            "Valid Loss : 0.897 Valid Acc  : 0.845\n",
            "Epoch: 10\n",
            "100% 112/112 [00:37<00:00,  2.99it/s]\n",
            "Train Loss : 0.870\n",
            "Valid Loss : 0.895 Valid Acc  : 0.847\n",
            "Epoch: 11\n",
            "100% 112/112 [00:37<00:00,  2.96it/s]\n",
            "Train Loss : 0.867\n",
            "Valid Loss : 0.889 Valid Acc  : 0.853\n",
            "Epoch: 12\n",
            "100% 112/112 [00:38<00:00,  2.93it/s]\n",
            "Train Loss : 0.861\n",
            "Valid Loss : 0.889 Valid Acc  : 0.853\n",
            "Epoch: 13\n",
            "100% 112/112 [00:37<00:00,  3.00it/s]\n",
            "Train Loss : 0.863\n",
            "Valid Loss : 0.891 Valid Acc  : 0.850\n",
            "Epoch: 14\n",
            "100% 112/112 [00:45<00:00,  2.49it/s]\n",
            "Train Loss : 0.854\n",
            "Valid Loss : 0.886 Valid Acc  : 0.855\n",
            "Epoch: 15\n",
            "100% 112/112 [00:38<00:00,  2.91it/s]\n",
            "Train Loss : 0.857\n",
            "Valid Loss : 0.882 Valid Acc  : 0.860\n",
            "\n",
            "Test Accuracy : 0.858\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.86      0.88      0.87      1900\n",
            "      Sports       0.94      0.91      0.92      1900\n",
            "    Business       0.82      0.81      0.82      1900\n",
            "    Sci/Tech       0.81      0.83      0.82      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1667   59   98   76]\n",
            " [  61 1734   28   77]\n",
            " [ 123   31 1540  206]\n",
            " [  88   30  206 1576]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 85.8%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 86.0%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2117996\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 38 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n2sy3IPv9vfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Χρήση LSTM"
      ],
      "metadata": {
        "id": "B4VaUn8_1Fm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1T1qYmhrj84",
        "outputId": "42fac36e-bac8-4186-ad5a-9c72b90072ab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 1: Αντικατάσταση του RNN με LSTM"
      ],
      "metadata": {
        "id": "zs4KIavuFXHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update paths in the RNN code\n",
        "updated_rnn_code = rnn_code.replace('ag-news-classification-dataset/train.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
        "updated_rnn_code = updated_rnn_code.replace('ag-news-classification-dataset/test.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
        "\n",
        "# Save the updated file to the correct location\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPbCE2tqcgd",
        "outputId": "683d58bb-8a11-489d-e039-740b5a270bec"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα αντικαταστήσουμε την αρχική χρήση του RNN με LSTM και θα κάνουμε τις απαραίτητες προσαρμογές στον κώδικα για να δουλέψει σωστά με το LSTM. Παρακάτω παρατίθεται το κομμάτι κώδικα που πρέπει να τροποποιηθεί για να χρησιμοποιηθεί LSTM αντί για RNN.\n",
        "\n",
        "Αλλαγές στον κώδικα:\n",
        "\n",
        "*   Αντικαθιστούμε το nn.RNN με nn.LSTM.\n",
        "*   Επειδή το LSTM έχει δύο εξόδους, h_n και c_n, πρέπει να προσαρμόσουμε την εξαγωγή της εξόδου για να χρησιμοποιήσουμε μόνο την πρώτη έξοδο (που είναι το h_n).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bB_Kgnz8Fcf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικατάσταση του RNN με LSTM\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)'\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "id": "qLl4K2Lh1G6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380c0acf-4392-4e65-c3a0-f88f7911f461"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R74ACC5GLyI",
        "outputId": "c23fb6a4-b609-492e-8db3-e1554edcc9e3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): LSTM(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2168156\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:35<00:00,  3.17it/s]\n",
            "Train Loss : 1.256\n",
            "Valid Loss : 1.046 Valid Acc  : 0.705\n",
            "Epoch: 2\n",
            "100% 112/112 [00:33<00:00,  3.31it/s]\n",
            "Train Loss : 0.976\n",
            "Valid Loss : 0.936 Valid Acc  : 0.809\n",
            "Epoch: 3\n",
            "100% 112/112 [00:33<00:00,  3.35it/s]\n",
            "Train Loss : 0.911\n",
            "Valid Loss : 0.903 Valid Acc  : 0.841\n",
            "Epoch: 4\n",
            "100% 112/112 [00:34<00:00,  3.24it/s]\n",
            "Train Loss : 0.882\n",
            "Valid Loss : 0.887 Valid Acc  : 0.859\n",
            "Epoch: 5\n",
            "100% 112/112 [00:34<00:00,  3.24it/s]\n",
            "Train Loss : 0.864\n",
            "Valid Loss : 0.880 Valid Acc  : 0.862\n",
            "Epoch: 6\n",
            "100% 112/112 [00:33<00:00,  3.34it/s]\n",
            "Train Loss : 0.851\n",
            "Valid Loss : 0.876 Valid Acc  : 0.866\n",
            "Epoch: 7\n",
            "100% 112/112 [00:34<00:00,  3.28it/s]\n",
            "Train Loss : 0.843\n",
            "Valid Loss : 0.874 Valid Acc  : 0.869\n",
            "Epoch: 8\n",
            "100% 112/112 [00:34<00:00,  3.27it/s]\n",
            "Train Loss : 0.836\n",
            "Valid Loss : 0.870 Valid Acc  : 0.871\n",
            "Epoch: 9\n",
            "100% 112/112 [00:33<00:00,  3.32it/s]\n",
            "Train Loss : 0.829\n",
            "Valid Loss : 0.868 Valid Acc  : 0.874\n",
            "Epoch: 10\n",
            "100% 112/112 [00:33<00:00,  3.33it/s]\n",
            "Train Loss : 0.825\n",
            "Valid Loss : 0.866 Valid Acc  : 0.875\n",
            "Epoch: 11\n",
            "100% 112/112 [00:34<00:00,  3.23it/s]\n",
            "Train Loss : 0.821\n",
            "Valid Loss : 0.869 Valid Acc  : 0.873\n",
            "Epoch: 12\n",
            "100% 112/112 [00:33<00:00,  3.35it/s]\n",
            "Train Loss : 0.818\n",
            "Valid Loss : 0.865 Valid Acc  : 0.876\n",
            "Epoch: 13\n",
            "100% 112/112 [00:33<00:00,  3.35it/s]\n",
            "Train Loss : 0.815\n",
            "Valid Loss : 0.861 Valid Acc  : 0.880\n",
            "Epoch: 14\n",
            "100% 112/112 [00:35<00:00,  3.17it/s]\n",
            "Train Loss : 0.813\n",
            "Valid Loss : 0.864 Valid Acc  : 0.878\n",
            "Epoch: 15\n",
            "100% 112/112 [00:33<00:00,  3.34it/s]\n",
            "Train Loss : 0.811\n",
            "Valid Loss : 0.864 Valid Acc  : 0.878\n",
            "\n",
            "Test Accuracy : 0.880\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.90      0.87      0.88      1900\n",
            "      Sports       0.93      0.95      0.94      1900\n",
            "    Business       0.85      0.83      0.84      1900\n",
            "    Sci/Tech       0.84      0.87      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1650   74   93   83]\n",
            " [  25 1803   41   31]\n",
            " [  78   32 1579  211]\n",
            " [  76   28  137 1659]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 88.0%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 87.8%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2168156\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 33.5 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aSIbVXYR-61_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 2: Ενημέρωση για Δι-κατευθυντήριο LSTM"
      ],
      "metadata": {
        "id": "GdQAShAXFrIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Όπως κάναμε για το RNN, θα προσθέσουμε την παράμετρο bidirectional=True για να το κάνουμε δι-κατευθυντήριο. Αυτό θα διπλασιάσει το μέγεθος του hidden state."
      ],
      "metadata": {
        "id": "e9XLop2gFvnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημέρωση για Δι-κατευθυντήριο LSTM\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημέρωση του γραμμικού layer για διπλασιασμένο μέγεθος εξόδου\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'\n",
        ")\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9LKpleuFw8u",
        "outputId": "cc6cc514-d7ca-4404-ff97-2a0bf70a256e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mowdBysYGKXW",
        "outputId": "937d4c5c-c028-4aa3-9571-4f3b58e7d027"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2210908\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [01:00<00:00,  1.85it/s]\n",
            "Train Loss : 1.251\n",
            "Valid Loss : 1.046 Valid Acc  : 0.703\n",
            "Epoch: 2\n",
            "100% 112/112 [01:00<00:00,  1.87it/s]\n",
            "Train Loss : 0.980\n",
            "Valid Loss : 0.945 Valid Acc  : 0.801\n",
            "Epoch: 3\n",
            "100% 112/112 [00:59<00:00,  1.88it/s]\n",
            "Train Loss : 0.914\n",
            "Valid Loss : 0.913 Valid Acc  : 0.832\n",
            "Epoch: 4\n",
            "100% 112/112 [00:59<00:00,  1.89it/s]\n",
            "Train Loss : 0.883\n",
            "Valid Loss : 0.898 Valid Acc  : 0.848\n",
            "Epoch: 5\n",
            "100% 112/112 [00:58<00:00,  1.91it/s]\n",
            "Train Loss : 0.865\n",
            "Valid Loss : 0.890 Valid Acc  : 0.852\n",
            "Epoch: 6\n",
            "100% 112/112 [01:02<00:00,  1.79it/s]\n",
            "Train Loss : 0.852\n",
            "Valid Loss : 0.882 Valid Acc  : 0.860\n",
            "Epoch: 7\n",
            "100% 112/112 [01:01<00:00,  1.82it/s]\n",
            "Train Loss : 0.843\n",
            "Valid Loss : 0.877 Valid Acc  : 0.865\n",
            "Epoch: 8\n",
            "100% 112/112 [01:02<00:00,  1.80it/s]\n",
            "Train Loss : 0.836\n",
            "Valid Loss : 0.874 Valid Acc  : 0.869\n",
            "Epoch: 9\n",
            "100% 112/112 [01:01<00:00,  1.83it/s]\n",
            "Train Loss : 0.830\n",
            "Valid Loss : 0.872 Valid Acc  : 0.870\n",
            "Epoch: 10\n",
            "100% 112/112 [01:00<00:00,  1.84it/s]\n",
            "Train Loss : 0.825\n",
            "Valid Loss : 0.870 Valid Acc  : 0.872\n",
            "Epoch: 11\n",
            "100% 112/112 [01:02<00:00,  1.80it/s]\n",
            "Train Loss : 0.821\n",
            "Valid Loss : 0.871 Valid Acc  : 0.871\n",
            "Epoch: 12\n",
            "100% 112/112 [00:59<00:00,  1.87it/s]\n",
            "Train Loss : 0.818\n",
            "Valid Loss : 0.868 Valid Acc  : 0.874\n",
            "Epoch: 13\n",
            "100% 112/112 [00:59<00:00,  1.88it/s]\n",
            "Train Loss : 0.815\n",
            "Valid Loss : 0.867 Valid Acc  : 0.874\n",
            "Epoch: 14\n",
            "100% 112/112 [01:00<00:00,  1.86it/s]\n",
            "Train Loss : 0.812\n",
            "Valid Loss : 0.865 Valid Acc  : 0.876\n",
            "Epoch: 15\n",
            "100% 112/112 [01:00<00:00,  1.86it/s]\n",
            "Train Loss : 0.810\n",
            "Valid Loss : 0.865 Valid Acc  : 0.878\n",
            "\n",
            "Test Accuracy : 0.879\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.92      0.86      0.88      1900\n",
            "      Sports       0.93      0.96      0.94      1900\n",
            "    Business       0.84      0.84      0.84      1900\n",
            "    Sci/Tech       0.84      0.86      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1627   73  107   93]\n",
            " [  25 1819   26   30]\n",
            " [  68   36 1595  201]\n",
            " [  57   32  171 1640]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 87.9%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 87.8%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,210,908\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 61.0 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jo082qLi_SDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 3: Ενημέρωση για Διπλό Στρώμα Δι-κατευθυντήριου LSTM"
      ],
      "metadata": {
        "id": "LOW-c7bxFz_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Όπως και με το RNN, θα προσθέσουμε και ένα δεύτερο επίπεδο στο LSTM και θα διπλασιάσουμε το μέγεθος του hidden state."
      ],
      "metadata": {
        "id": "q_cM4W_NF2g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημέρωση για Διπλό Στρώμα Δι-κατευθυντήριο LSTM\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημέρωση του γραμμικού layer για διπλό στρώμα\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'logits = self.linear(output[:, -1])',\n",
        "    'logits = self.linear(output[:, -1, :])'  # Για δύο στρώματα δι-κατευθυντήριου RNN, το μέγεθος είναι hidden_dim * 2 * 2\n",
        ")\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8H24ZfNFsHw",
        "outputId": "e1725bd1-4858-491d-ac08-415b84a2e145"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuT-JjXsGH6h",
        "outputId": "1cdedcec-eccf-4a3e-cc70-b820093a30f3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2310236\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [01:52<00:00,  1.00s/it]\n",
            "Train Loss : 1.202\n",
            "Valid Loss : 1.025 Valid Acc  : 0.713\n",
            "Epoch: 2\n",
            "100% 112/112 [01:50<00:00,  1.01it/s]\n",
            "Train Loss : 0.956\n",
            "Valid Loss : 0.928 Valid Acc  : 0.813\n",
            "Epoch: 3\n",
            "100% 112/112 [01:53<00:00,  1.01s/it]\n",
            "Train Loss : 0.897\n",
            "Valid Loss : 0.893 Valid Acc  : 0.847\n",
            "Epoch: 4\n",
            "100% 112/112 [01:52<00:00,  1.01s/it]\n",
            "Train Loss : 0.874\n",
            "Valid Loss : 0.883 Valid Acc  : 0.859\n",
            "Epoch: 5\n",
            "100% 112/112 [01:52<00:00,  1.01s/it]\n",
            "Train Loss : 0.856\n",
            "Valid Loss : 0.875 Valid Acc  : 0.866\n",
            "Epoch: 6\n",
            "100% 112/112 [01:55<00:00,  1.03s/it]\n",
            "Train Loss : 0.845\n",
            "Valid Loss : 0.868 Valid Acc  : 0.875\n",
            "Epoch: 7\n",
            "100% 112/112 [01:51<00:00,  1.01it/s]\n",
            "Train Loss : 0.838\n",
            "Valid Loss : 0.872 Valid Acc  : 0.869\n",
            "Epoch: 8\n",
            "100% 112/112 [01:51<00:00,  1.01it/s]\n",
            "Train Loss : 0.832\n",
            "Valid Loss : 0.866 Valid Acc  : 0.875\n",
            "Epoch: 9\n",
            "100% 112/112 [01:49<00:00,  1.02it/s]\n",
            "Train Loss : 0.826\n",
            "Valid Loss : 0.865 Valid Acc  : 0.877\n",
            "Epoch: 10\n",
            "100% 112/112 [01:50<00:00,  1.01it/s]\n",
            "Train Loss : 0.823\n",
            "Valid Loss : 0.860 Valid Acc  : 0.881\n",
            "Epoch: 11\n",
            "100% 112/112 [01:50<00:00,  1.01it/s]\n",
            "Train Loss : 0.819\n",
            "Valid Loss : 0.858 Valid Acc  : 0.884\n",
            "Epoch: 12\n",
            "100% 112/112 [01:52<00:00,  1.00s/it]\n",
            "Train Loss : 0.815\n",
            "Valid Loss : 0.858 Valid Acc  : 0.883\n",
            "Epoch: 13\n",
            "100% 112/112 [01:52<00:00,  1.00s/it]\n",
            "Train Loss : 0.813\n",
            "Valid Loss : 0.857 Valid Acc  : 0.885\n",
            "Epoch: 14\n",
            "100% 112/112 [01:53<00:00,  1.01s/it]\n",
            "Train Loss : 0.811\n",
            "Valid Loss : 0.857 Valid Acc  : 0.886\n",
            "Epoch: 15\n",
            "100% 112/112 [01:51<00:00,  1.01it/s]\n",
            "Train Loss : 0.810\n",
            "Valid Loss : 0.857 Valid Acc  : 0.885\n",
            "\n",
            "Test Accuracy : 0.883\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.89      0.89      0.89      1900\n",
            "      Sports       0.93      0.96      0.94      1900\n",
            "    Business       0.85      0.83      0.84      1900\n",
            "    Sci/Tech       0.86      0.85      0.86      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1698   58   92   52]\n",
            " [  29 1816   36   19]\n",
            " [  89   42 1586  183]\n",
            " [ 101   40  146 1613]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 88.3%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 88.5%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,310,236\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 1 λεπτό και 51 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "i_4bCYMn_eGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5:"
      ],
      "metadata": {
        "id": "gX3LsAFGJwgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Δημιουργία του DataFrame με τα δεδομένα\n",
        "data = {\n",
        "    'Μοντέλο': ['1RNN', '1Bi-RNN', '2Bi-RNN', '1LSTM', '1Bi-LSTM', '2Bi-LSTM'],\n",
        "    'Accuracy (Test Set)': ['86.5%', '85.3%', '85.8%', '88.0%', '87.9%', '88.3%'],\n",
        "    'Accuracy (Validation Set)': ['87.0%', '86.6%', '86.0%', '87.8%', '87.8%', '88.5%'],\n",
        "    'Πλήθος Παραμέτρων': [2136284, 2147164, 2117996, 2168156, 2210908, 2310236],\n",
        "    'Χρόνος Εκπαίδευσης ανά Εποχή': ['16 δευτερόλεπτα', '25 δευτερόλεπτα', '38 δευτερόλεπτα',\n",
        "                                    '33.5 δευτερόλεπτα', '61.0 δευτερόλεπτα', '1 λεπτό 51 δευτερόλεπτα']\n",
        "}\n",
        "\n",
        "# Δημιουργία DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Εμφάνιση του πίνακα\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "yHt424tQCBOJ",
        "outputId": "510230bf-d8c6-4f9c-c82e-cfa7bfedcd7f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Μοντέλο Accuracy (Test Set) Accuracy (Validation Set)  Πλήθος Παραμέτρων  \\\n",
              "0      1RNN               86.5%                     87.0%            2136284   \n",
              "1   1Bi-RNN               85.3%                     86.6%            2147164   \n",
              "2   2Bi-RNN               85.8%                     86.0%            2117996   \n",
              "3     1LSTM               88.0%                     87.8%            2168156   \n",
              "4  1Bi-LSTM               87.9%                     87.8%            2210908   \n",
              "5  2Bi-LSTM               88.3%                     88.5%            2310236   \n",
              "\n",
              "  Χρόνος Εκπαίδευσης ανά Εποχή  \n",
              "0              16 δευτερόλεπτα  \n",
              "1              25 δευτερόλεπτα  \n",
              "2              38 δευτερόλεπτα  \n",
              "3            33.5 δευτερόλεπτα  \n",
              "4            61.0 δευτερόλεπτα  \n",
              "5      1 λεπτό 51 δευτερόλεπτα  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a6c0549-c45d-44fe-b867-632c97f83697\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Μοντέλο</th>\n",
              "      <th>Accuracy (Test Set)</th>\n",
              "      <th>Accuracy (Validation Set)</th>\n",
              "      <th>Πλήθος Παραμέτρων</th>\n",
              "      <th>Χρόνος Εκπαίδευσης ανά Εποχή</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1RNN</td>\n",
              "      <td>86.5%</td>\n",
              "      <td>87.0%</td>\n",
              "      <td>2136284</td>\n",
              "      <td>16 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1Bi-RNN</td>\n",
              "      <td>85.3%</td>\n",
              "      <td>86.6%</td>\n",
              "      <td>2147164</td>\n",
              "      <td>25 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2Bi-RNN</td>\n",
              "      <td>85.8%</td>\n",
              "      <td>86.0%</td>\n",
              "      <td>2117996</td>\n",
              "      <td>38 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1LSTM</td>\n",
              "      <td>88.0%</td>\n",
              "      <td>87.8%</td>\n",
              "      <td>2168156</td>\n",
              "      <td>33.5 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1Bi-LSTM</td>\n",
              "      <td>87.9%</td>\n",
              "      <td>87.8%</td>\n",
              "      <td>2210908</td>\n",
              "      <td>61.0 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2Bi-LSTM</td>\n",
              "      <td>88.3%</td>\n",
              "      <td>88.5%</td>\n",
              "      <td>2310236</td>\n",
              "      <td>1 λεπτό 51 δευτερόλεπτα</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a6c0549-c45d-44fe-b867-632c97f83697')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a6c0549-c45d-44fe-b867-632c97f83697 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a6c0549-c45d-44fe-b867-632c97f83697');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbb2e1fa-0c8e-4e16-a3ba-3ae92b0a47c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbb2e1fa-0c8e-4e16-a3ba-3ae92b0a47c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbb2e1fa-0c8e-4e16-a3ba-3ae92b0a47c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c9ecf75a-28ff-4da8-b556-ca06e5f76502\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c9ecf75a-28ff-4da8-b556-ca06e5f76502 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"\\u039c\\u03bf\\u03bd\\u03c4\\u03ad\\u03bb\\u03bf\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"1RNN\",\n          \"1Bi-RNN\",\n          \"2Bi-LSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy (Test Set)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"86.5%\",\n          \"85.3%\",\n          \"88.3%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy (Validation Set)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"86.6%\",\n          \"88.5%\",\n          \"86.0%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u03a0\\u03bb\\u03ae\\u03b8\\u03bf\\u03c2 \\u03a0\\u03b1\\u03c1\\u03b1\\u03bc\\u03ad\\u03c4\\u03c1\\u03c9\\u03bd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70528,\n        \"min\": 2117996,\n        \"max\": 2310236,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          2136284,\n          2147164,\n          2310236\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u03a7\\u03c1\\u03cc\\u03bd\\u03bf\\u03c2 \\u0395\\u03ba\\u03c0\\u03b1\\u03af\\u03b4\\u03b5\\u03c5\\u03c3\\u03b7\\u03c2 \\u03b1\\u03bd\\u03ac \\u0395\\u03c0\\u03bf\\u03c7\\u03ae\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"16 \\u03b4\\u03b5\\u03c5\\u03c4\\u03b5\\u03c1\\u03cc\\u03bb\\u03b5\\u03c0\\u03c4\\u03b1\",\n          \"25 \\u03b4\\u03b5\\u03c5\\u03c4\\u03b5\\u03c1\\u03cc\\u03bb\\u03b5\\u03c0\\u03c4\\u03b1\",\n          \"1 \\u03bb\\u03b5\\u03c0\\u03c4\\u03cc 51 \\u03b4\\u03b5\\u03c5\\u03c4\\u03b5\\u03c1\\u03cc\\u03bb\\u03b5\\u03c0\\u03c4\\u03b1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|index|Μοντέλο|Accuracy \\(Test Set\\)|Accuracy \\(Validation Set\\)|Πλήθος Παραμέτρων|Χρόνος Εκπαίδευσης ανά Εποχή|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1RNN|86\\.5%|87\\.0%|2136284|16 δευτερόλεπτα|\n",
        "|1|1Bi-RNN|85\\.3%|86\\.6%|2147164|25 δευτερόλεπτα|\n",
        "|2|2Bi-RNN|85\\.8%|86\\.0%|2117996|38 δευτερόλεπτα|\n",
        "|3|1LSTM|88\\.0%|87\\.8%|2168156|33\\.5 δευτερόλεπτα|\n",
        "|4|1Bi-LSTM|87\\.9%|87\\.8%|2210908|61\\.0 δευτερόλεπτα|\n",
        "|5|2Bi-LSTM|88\\.3%|88\\.5%|2310236|1 λεπτό 51 δευτερόλεπτα|"
      ],
      "metadata": {
        "id": "t7BDOffiBbAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Πόσο επηρεάζεται η επίδοση του δικτύου RNN/LSTM από την επιλογή του μοντέλου;"
      ],
      "metadata": {
        "id": "g3ykjcWbCdFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η επίδοση του δικτύου επηρεάζεται σημαντικά από την επιλογή του μοντέλου. Τα αποτελέσματα δείχνουν ότι τα LSTM μοντέλα (και ιδιαίτερα τα Bi-LSTM) υπερέχουν σε σχέση με τα απλά RNN μοντέλα:\n",
        "\n",
        "\n",
        "*   Τα LSTM μοντέλα (1LSTM, 1Bi-LSTM, 2Bi-LSTM) εμφανίζουν υψηλότερη ακρίβεια στο test set σε σχέση με τα RNN μοντέλα (1RNN, 1Bi-RNN, 2Bi-RNN). Για παράδειγμα, το 2Bi-LSTM πετυχαίνει 88.3% στην ακρίβεια του test set, ενώ το 2Bi-RNN φτάνει μόνο το 85.8%.\n",
        "*   Τα Bi-LSTM μοντέλα φαίνεται να είναι πιο ισχυρά στην κατανόηση της σειράς και της εξάρτησης στα δεδομένα, κάτι που οδηγεί σε καλύτερη απόδοση.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qgjpLSF3Cncm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Πόσο επηρεάζεται η πολυπλοκότητα (παράμετροι, χρόνος εκπαίδευσης) από την επιλογή του μοντέλου;"
      ],
      "metadata": {
        "id": "EYAh0YO1CqBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η πολυπλοκότητα (τόσο ο αριθμός παραμέτρων όσο και ο χρόνος εκπαίδευσης) αυξάνεται καθώς μεταβαίνουμε από τα RNN στα LSTM μοντέλα και από τα 1RNN/1LSTM στα 2Bi-RNN/2Bi-LSTM:\n",
        "\n",
        "\n",
        "*   Πλήθος παραμέτρων: Τα Bi-LSTM μοντέλα έχουν περισσότερες παραμέτρους. Για παράδειγμα, το 2Bi-LSTM έχει 2,310,236 παραμέτρους, ενώ το 1RNN έχει 2,136,284 παραμέτρους.\n",
        "*   Χρόνος εκπαίδευσης: Ο χρόνος εκπαίδευσης ανά εποχή είναι μεγαλύτερος για τα Bi-LSTM μοντέλα. Για παράδειγμα, το 2Bi-LSTM χρειάζεται 1 λεπτό και 51 δευτερόλεπτα ανά εποχή, ενώ το 1RNN χρειάζεται μόνο 16 δευτερόλεπτα ανά εποχή.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Αυτό υποδηλώνει ότι η χρήση των Bi-LSTM μοντέλων αυξάνει την πολυπλοκότητα, αλλά ταυτόχρονα βελτιώνει την επίδοση."
      ],
      "metadata": {
        "id": "vDtBkkO6Csw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Διαφέρει σημαντικά η επίδοση στο test set και στο validation set;"
      ],
      "metadata": {
        "id": "gjB42cbkCznn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η διαφορά μεταξύ test set και validation set είναι μικρή σε όλα τα μοντέλα, τόσο για τα RNN όσο και για τα LSTM:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Η διαφορά στην ακρίβεια κυμαίνεται γύρω από το 0.3% - 0.5% σε όλα τα μοντέλα.\n",
        "*   Αυτή η μικρή διαφορά υποδηλώνει ότι τα μοντέλα γενικά γενικεύουν καλά στα δεδομένα του test set, χωρίς υπερβολική εξειδίκευση στα δεδομένα εκπαίδευσης.\n",
        "\n"
      ],
      "metadata": {
        "id": "wfMJy1nJCu8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Ποιο είδος μοντέλου φαίνεται να υπερέχει, το RNN ή το LSTM;"
      ],
      "metadata": {
        "id": "GstDftIgC4DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το LSTM υπερέχει ξεκάθαρα σε σχέση με το RNN όσον αφορά την ακρίβεια και την απόδοση:\n",
        "\n",
        "\n",
        "*   Τα LSTM μοντέλα (1LSTM, 1Bi-LSTM, 2Bi-LSTM) εμφανίζουν σημαντικά καλύτερη επίδοση σε σύγκριση με τα RNN μοντέλα.\n",
        "*   Ο λόγος είναι ότι τα LSTM είναι ικανά να διατηρούν μακροπρόθεσμες εξαρτήσεις και να επιλύουν τα προβλήματα της εξαφάνισης του βαθμού (vanishing gradient), κάτι που τους επιτρέπει να έχουν καλύτερη απόδοση στις εργασίες ταξινόμησης.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E1vM0D36C6TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6:"
      ],
      "metadata": {
        "id": "N7YZYi7EJ2TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulf9NvcY7wAe",
        "outputId": "de9b0ff7-0b50-4356-8fbc-107948c2b287"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update paths in the RNN code\n",
        "updated_rnn_code = rnn_code.replace('ag-news-classification-dataset/train.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
        "updated_rnn_code = updated_rnn_code.replace('ag-news-classification-dataset/test.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
        "\n",
        "# Save the updated file to the correct location\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVYzj41V0NGV",
        "outputId": "908693bc-f6cd-43c2-9745-6441113bc27f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Βήμα 1: Ενημέρωση της παραμέτρου MAX_WORDS στα μοντέλα"
      ],
      "metadata": {
        "id": "EzgJ7FjHk4dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Αλλαγή της παραμέτρου MAX_WORDS\n",
        "updated_rnn_code = rnn_code.replace('MAX_WORDS = 25', 'MAX_WORDS = 50')\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX9zUT_kJ305",
        "outputId": "c0c7490c-eafe-4cc6-db57-b90d077e6abf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 50\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Βήμα 2: Εκτέλεση των μοντέλων RNN (απλό και δι-κατευθυντήριο)"
      ],
      "metadata": {
        "id": "8vNiJgKRlBaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Εκτέλεση για απλό RNN:"
      ],
      "metadata": {
        "id": "XChrfWimlIbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln3jV31UlNQS",
        "outputId": "4f6ef36d-b2fc-47bf-b9bf-ccc2eff60ed0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:25<00:00,  4.48it/s]\n",
            "Train Loss : 1.380\n",
            "Valid Loss : 1.346 Valid Acc  : 0.346\n",
            "Epoch: 2\n",
            "100% 112/112 [00:25<00:00,  4.42it/s]\n",
            "Train Loss : 1.329\n",
            "Valid Loss : 1.312 Valid Acc  : 0.397\n",
            "Epoch: 3\n",
            "100% 112/112 [00:25<00:00,  4.33it/s]\n",
            "Train Loss : 1.302\n",
            "Valid Loss : 1.311 Valid Acc  : 0.395\n",
            "Epoch: 4\n",
            "100% 112/112 [00:25<00:00,  4.37it/s]\n",
            "Train Loss : 1.324\n",
            "Valid Loss : 1.312 Valid Acc  : 0.386\n",
            "Epoch: 5\n",
            "100% 112/112 [00:24<00:00,  4.55it/s]\n",
            "Train Loss : 1.288\n",
            "Valid Loss : 1.311 Valid Acc  : 0.361\n",
            "Epoch: 6\n",
            "100% 112/112 [00:26<00:00,  4.29it/s]\n",
            "Train Loss : 1.294\n",
            "Valid Loss : 1.301 Valid Acc  : 0.396\n",
            "Epoch: 7\n",
            "100% 112/112 [00:25<00:00,  4.33it/s]\n",
            "Train Loss : 1.304\n",
            "Valid Loss : 1.296 Valid Acc  : 0.406\n",
            "Epoch: 8\n",
            "100% 112/112 [00:26<00:00,  4.28it/s]\n",
            "Train Loss : 1.281\n",
            "Valid Loss : 1.329 Valid Acc  : 0.349\n",
            "Epoch: 9\n",
            "100% 112/112 [00:31<00:00,  3.61it/s]\n",
            "Train Loss : 1.329\n",
            "Valid Loss : 1.328 Valid Acc  : 0.371\n",
            "Epoch: 10\n",
            "100% 112/112 [00:24<00:00,  4.59it/s]\n",
            "Train Loss : 1.318\n",
            "Valid Loss : 1.313 Valid Acc  : 0.389\n",
            "Epoch: 11\n",
            "100% 112/112 [00:25<00:00,  4.33it/s]\n",
            "Train Loss : 1.296\n",
            "Valid Loss : 1.278 Valid Acc  : 0.433\n",
            "Epoch: 12\n",
            "100% 112/112 [00:26<00:00,  4.27it/s]\n",
            "Train Loss : 1.271\n",
            "Valid Loss : 1.276 Valid Acc  : 0.433\n",
            "Epoch: 13\n",
            "100% 112/112 [00:26<00:00,  4.21it/s]\n",
            "Train Loss : 1.310\n",
            "Valid Loss : 1.332 Valid Acc  : 0.368\n",
            "Epoch: 14\n",
            "100% 112/112 [00:25<00:00,  4.48it/s]\n",
            "Train Loss : 1.328\n",
            "Valid Loss : 1.329 Valid Acc  : 0.373\n",
            "Epoch: 15\n",
            "100% 112/112 [00:25<00:00,  4.34it/s]\n",
            "Train Loss : 1.320\n",
            "Valid Loss : 1.322 Valid Acc  : 0.382\n",
            "\n",
            "Test Accuracy : 0.378\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.42      0.68      0.52      1900\n",
            "      Sports       0.35      0.71      0.47      1900\n",
            "    Business       0.50      0.01      0.03      1900\n",
            "    Sci/Tech       0.34      0.11      0.17      1900\n",
            "\n",
            "    accuracy                           0.38      7600\n",
            "   macro avg       0.40      0.38      0.29      7600\n",
            "weighted avg       0.40      0.38      0.29      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1299  495   15   91]\n",
            " [ 394 1343    3  160]\n",
            " [ 907  823   25  145]\n",
            " [ 472 1213    7  208]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 37.8%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 38.2%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,136,284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 25 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4JKr_P3_t3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Τροποποίηση για δι-κατευθυντήριο RNN:"
      ],
      "metadata": {
        "id": "Ip6B3eyWlUQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'\n",
        ")\n"
      ],
      "metadata": {
        "id": "u1lHkGgMlXBh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQExuntYlY0L",
        "outputId": "1a278fb4-e965-4518-ffca-e44b0877f625"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:26<00:00,  4.25it/s]\n",
            "Train Loss : 1.383\n",
            "Valid Loss : 1.379 Valid Acc  : 0.295\n",
            "Epoch: 2\n",
            "100% 112/112 [00:25<00:00,  4.44it/s]\n",
            "Train Loss : 1.339\n",
            "Valid Loss : 1.294 Valid Acc  : 0.412\n",
            "Epoch: 3\n",
            "100% 112/112 [00:28<00:00,  3.93it/s]\n",
            "Train Loss : 1.303\n",
            "Valid Loss : 1.298 Valid Acc  : 0.409\n",
            "Epoch: 4\n",
            "100% 112/112 [00:24<00:00,  4.49it/s]\n",
            "Train Loss : 1.286\n",
            "Valid Loss : 1.285 Valid Acc  : 0.427\n",
            "Epoch: 5\n",
            "100% 112/112 [00:25<00:00,  4.40it/s]\n",
            "Train Loss : 1.287\n",
            "Valid Loss : 1.349 Valid Acc  : 0.336\n",
            "Epoch: 6\n",
            "100% 112/112 [00:25<00:00,  4.34it/s]\n",
            "Train Loss : 1.277\n",
            "Valid Loss : 1.256 Valid Acc  : 0.451\n",
            "Epoch: 7\n",
            "100% 112/112 [00:26<00:00,  4.30it/s]\n",
            "Train Loss : 1.235\n",
            "Valid Loss : 1.235 Valid Acc  : 0.460\n",
            "Epoch: 8\n",
            "100% 112/112 [00:26<00:00,  4.22it/s]\n",
            "Train Loss : 1.216\n",
            "Valid Loss : 1.131 Valid Acc  : 0.617\n",
            "Epoch: 9\n",
            "100% 112/112 [00:26<00:00,  4.30it/s]\n",
            "Train Loss : 1.271\n",
            "Valid Loss : 1.350 Valid Acc  : 0.351\n",
            "Epoch: 10\n",
            "100% 112/112 [00:25<00:00,  4.36it/s]\n",
            "Train Loss : 1.345\n",
            "Valid Loss : 1.346 Valid Acc  : 0.356\n",
            "Epoch: 11\n",
            "100% 112/112 [00:26<00:00,  4.28it/s]\n",
            "Train Loss : 1.329\n",
            "Valid Loss : 1.320 Valid Acc  : 0.391\n",
            "Epoch: 12\n",
            "100% 112/112 [00:25<00:00,  4.46it/s]\n",
            "Train Loss : 1.317\n",
            "Valid Loss : 1.365 Valid Acc  : 0.358\n",
            "Epoch: 13\n",
            "100% 112/112 [00:26<00:00,  4.25it/s]\n",
            "Train Loss : 1.338\n",
            "Valid Loss : 1.306 Valid Acc  : 0.423\n",
            "Epoch: 14\n",
            "100% 112/112 [00:30<00:00,  3.72it/s]\n",
            "Train Loss : 1.336\n",
            "Valid Loss : 1.339 Valid Acc  : 0.347\n",
            "Epoch: 15\n",
            "100% 112/112 [00:30<00:00,  3.72it/s]\n",
            "Train Loss : 1.326\n",
            "Valid Loss : 1.314 Valid Acc  : 0.398\n",
            "\n",
            "Test Accuracy : 0.391\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.48      0.22      0.31      1900\n",
            "      Sports       0.49      0.40      0.44      1900\n",
            "    Business       0.31      0.66      0.42      1900\n",
            "    Sci/Tech       0.48      0.27      0.35      1900\n",
            "\n",
            "    accuracy                           0.39      7600\n",
            "   macro avg       0.44      0.39      0.38      7600\n",
            "weighted avg       0.44      0.39      0.38      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[ 426  280 1134   60]\n",
            " [ 132  769  741  258]\n",
            " [ 196  203 1261  240]\n",
            " [ 139  330  916  515]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 39.1%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 39.8%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,136,284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 26 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SefJfmgk_5B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Βήμα 3: Εκτέλεση των μοντέλων LSTM (απλό και δι-κατευθυντήριο)"
      ],
      "metadata": {
        "id": "UcjUn52Rlb-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τροποποίηση για απλό LSTM: Αλλάξτε το RNN σε LSTM στο αρχείο:"
      ],
      "metadata": {
        "id": "lkR3epoxlfOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_rnn_code = updated_rnn_code.replace('self.rnn = nn.RNN', 'self.rnn = nn.LSTM')\n"
      ],
      "metadata": {
        "id": "vxKqj9pPllIs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Εκτέλεση για απλό LSTM:"
      ],
      "metadata": {
        "id": "R9YDktC1loA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNyq6Xjxlpln",
        "outputId": "86228922-d024-4bb4-a13e-bd439ccdc642"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:26<00:00,  4.28it/s]\n",
            "Train Loss : 1.383\n",
            "Valid Loss : 1.380 Valid Acc  : 0.279\n",
            "Epoch: 2\n",
            "100% 112/112 [00:29<00:00,  3.84it/s]\n",
            "Train Loss : 1.354\n",
            "Valid Loss : 1.360 Valid Acc  : 0.325\n",
            "Epoch: 3\n",
            "100% 112/112 [00:37<00:00,  3.02it/s]\n",
            "Train Loss : 1.369\n",
            "Valid Loss : 1.361 Valid Acc  : 0.312\n",
            "Epoch: 4\n",
            "100% 112/112 [00:30<00:00,  3.61it/s]\n",
            "Train Loss : 1.367\n",
            "Valid Loss : 1.358 Valid Acc  : 0.328\n",
            "Epoch: 5\n",
            "100% 112/112 [00:26<00:00,  4.20it/s]\n",
            "Train Loss : 1.363\n",
            "Valid Loss : 1.355 Valid Acc  : 0.333\n",
            "Epoch: 6\n",
            "100% 112/112 [00:25<00:00,  4.40it/s]\n",
            "Train Loss : 1.357\n",
            "Valid Loss : 1.359 Valid Acc  : 0.328\n",
            "Epoch: 7\n",
            "100% 112/112 [00:25<00:00,  4.41it/s]\n",
            "Train Loss : 1.361\n",
            "Valid Loss : 1.358 Valid Acc  : 0.330\n",
            "Epoch: 8\n",
            "100% 112/112 [00:25<00:00,  4.32it/s]\n",
            "Train Loss : 1.356\n",
            "Valid Loss : 1.317 Valid Acc  : 0.384\n",
            "Epoch: 9\n",
            "100% 112/112 [00:26<00:00,  4.24it/s]\n",
            "Train Loss : 1.312\n",
            "Valid Loss : 1.315 Valid Acc  : 0.369\n",
            "Epoch: 10\n",
            "100% 112/112 [00:26<00:00,  4.16it/s]\n",
            "Train Loss : 1.332\n",
            "Valid Loss : 1.300 Valid Acc  : 0.395\n",
            "Epoch: 11\n",
            "100% 112/112 [00:41<00:00,  2.70it/s]\n",
            "Train Loss : 1.316\n",
            "Valid Loss : 1.352 Valid Acc  : 0.335\n",
            "Epoch: 12\n",
            "100% 112/112 [01:14<00:00,  1.49it/s]\n",
            "Train Loss : 1.340\n",
            "Valid Loss : 1.327 Valid Acc  : 0.352\n",
            "Epoch: 13\n",
            "100% 112/112 [00:27<00:00,  4.09it/s]\n",
            "Train Loss : 1.307\n",
            "Valid Loss : 1.297 Valid Acc  : 0.383\n",
            "Epoch: 14\n",
            "100% 112/112 [00:24<00:00,  4.64it/s]\n",
            "Train Loss : 1.284\n",
            "Valid Loss : 1.290 Valid Acc  : 0.410\n",
            "Epoch: 15\n",
            "100% 112/112 [00:26<00:00,  4.26it/s]\n",
            "Train Loss : 1.285\n",
            "Valid Loss : 1.368 Valid Acc  : 0.309\n",
            "\n",
            "Test Accuracy : 0.312\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.53      0.14      0.22      1900\n",
            "      Sports       0.28      0.84      0.42      1900\n",
            "    Business       0.36      0.21      0.26      1900\n",
            "    Sci/Tech       0.40      0.06      0.11      1900\n",
            "\n",
            "    accuracy                           0.31      7600\n",
            "   macro avg       0.39      0.31      0.25      7600\n",
            "weighted avg       0.39      0.31      0.25      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[ 268 1313  254   65]\n",
            " [  89 1592  176   43]\n",
            " [  65 1364  391   80]\n",
            " [  82 1443  252  123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 31.2%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 31.9%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,136,284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 29.2 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iH8ScWdSEBtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τροποποίηση για δι-κατευθυντήριο LSTM:"
      ],
      "metadata": {
        "id": "jdje1wE2lrUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'\n",
        ")\n"
      ],
      "metadata": {
        "id": "C4MPKqd9luI3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6_1An1UlwRy",
        "outputId": "984e1248-b712-4fa0-8aa8-d556a14900de"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:26<00:00,  4.24it/s]\n",
            "Train Loss : 1.380\n",
            "Valid Loss : 1.351 Valid Acc  : 0.331\n",
            "Epoch: 2\n",
            "100% 112/112 [00:24<00:00,  4.52it/s]\n",
            "Train Loss : 1.360\n",
            "Valid Loss : 1.363 Valid Acc  : 0.314\n",
            "Epoch: 3\n",
            "100% 112/112 [00:24<00:00,  4.50it/s]\n",
            "Train Loss : 1.355\n",
            "Valid Loss : 1.361 Valid Acc  : 0.328\n",
            "Epoch: 4\n",
            "100% 112/112 [00:26<00:00,  4.21it/s]\n",
            "Train Loss : 1.333\n",
            "Valid Loss : 1.315 Valid Acc  : 0.384\n",
            "Epoch: 5\n",
            "100% 112/112 [00:25<00:00,  4.43it/s]\n",
            "Train Loss : 1.372\n",
            "Valid Loss : 1.370 Valid Acc  : 0.297\n",
            "Epoch: 6\n",
            "100% 112/112 [00:24<00:00,  4.62it/s]\n",
            "Train Loss : 1.311\n",
            "Valid Loss : 1.260 Valid Acc  : 0.428\n",
            "Epoch: 7\n",
            "100% 112/112 [00:25<00:00,  4.35it/s]\n",
            "Train Loss : 1.323\n",
            "Valid Loss : 1.353 Valid Acc  : 0.341\n",
            "Epoch: 8\n",
            "100% 112/112 [00:25<00:00,  4.34it/s]\n",
            "Train Loss : 1.327\n",
            "Valid Loss : 1.335 Valid Acc  : 0.369\n",
            "Epoch: 9\n",
            "100% 112/112 [00:23<00:00,  4.67it/s]\n",
            "Train Loss : 1.274\n",
            "Valid Loss : 1.189 Valid Acc  : 0.549\n",
            "Epoch: 10\n",
            "100% 112/112 [00:25<00:00,  4.34it/s]\n",
            "Train Loss : 1.218\n",
            "Valid Loss : 1.210 Valid Acc  : 0.512\n",
            "Epoch: 11\n",
            "100% 112/112 [00:26<00:00,  4.20it/s]\n",
            "Train Loss : 1.168\n",
            "Valid Loss : 1.159 Valid Acc  : 0.586\n",
            "Epoch: 12\n",
            "100% 112/112 [00:24<00:00,  4.56it/s]\n",
            "Train Loss : 1.142\n",
            "Valid Loss : 1.318 Valid Acc  : 0.420\n",
            "Epoch: 13\n",
            "100% 112/112 [00:25<00:00,  4.35it/s]\n",
            "Train Loss : 1.296\n",
            "Valid Loss : 1.269 Valid Acc  : 0.465\n",
            "Epoch: 14\n",
            "100% 112/112 [00:25<00:00,  4.33it/s]\n",
            "Train Loss : 1.221\n",
            "Valid Loss : 1.256 Valid Acc  : 0.485\n",
            "Epoch: 15\n",
            "100% 112/112 [00:24<00:00,  4.65it/s]\n",
            "Train Loss : 1.205\n",
            "Valid Loss : 1.149 Valid Acc  : 0.594\n",
            "\n",
            "Test Accuracy : 0.599\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.59      0.71      0.65      1900\n",
            "      Sports       0.59      0.72      0.65      1900\n",
            "    Business       0.58      0.70      0.64      1900\n",
            "    Sci/Tech       0.71      0.27      0.39      1900\n",
            "\n",
            "    accuracy                           0.60      7600\n",
            "   macro avg       0.62      0.60      0.58      7600\n",
            "weighted avg       0.62      0.60      0.58      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1345  340  190   25]\n",
            " [ 419 1373   71   37]\n",
            " [ 254  165 1331  150]\n",
            " [ 243  466  684  507]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 59.9%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 59.4%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2,136,284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 25.3 δευτερόλεπτα\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tYw25moyFqrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η αλλαγή από απλό RNN σε δι-κατευθυντικό RNN ή LSTM βελτιώνει την απόδοση, καθώς επιτρέπει στο μοντέλο να κατανοήσει καλύτερα τις εξαρτήσεις στα δεδομένα, ειδικά σε ακολουθίες με μακροχρόνιες εξαρτήσεις. Η διπλή κατεύθυνση (bidirectional) αυξάνει την ακρίβεια, όπως φαίνεται από τα αποτελέσματα (π.χ., από 37.8% σε 39.1% για το RNN). Τα LSTM προσφέρουν καλύτερη απόδοση από τα RNN, ειδικά με την προσθήκη της διπλής κατεύθυνσης, αυξάνοντας την ακρίβεια σημαντικά (από 31.2% σε 59.9%).\n",
        "\n",
        "Ωστόσο, αυτή η βελτίωση στην απόδοση συνοδεύεται από αυξημένη πολυπλοκότητα, καθώς απαιτούν περισσότερους υπολογιστικούς πόρους και μεγαλύτερο χρόνο εκπαίδευσης."
      ],
      "metadata": {
        "id": "xiAFKymFGy_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αν παρατηρήσουμε τα αποτελέσματα:\n",
        "\n",
        "\n",
        "*   RNN (απλό): Προσφέρει σχετικά χαμηλή ακρίβεια (37.8%), αλλά είναι το λιγότερο υπολογιστικά απαιτητικό.\n",
        "*   Δι-κατευθυντικό RNN: Η χρήση διπλής κατεύθυνσης αυξάνει την ακρίβεια (39.1%), αν και παραμένει σχετικά χαμηλή.\n",
        "*   LSTM (απλό): Εντυπωσιακή πτώση στην απόδοση με ακρίβεια 31.2%. Παρόλο που τα LSTM θεωρούνται καλύτερα για μεγάλες ακολουθίες, φαίνεται ότι το μοντέλο δεν αποδίδει τόσο καλά στο συγκεκριμένο πρόβλημα.\n",
        "*   Δι-κατευθυντικό LSTM: Το καλύτερο αποτέλεσμα με σημαντική αύξηση στην ακρίβεια (59.9%), επιβεβαιώνοντας ότι η χρήση LSTM σε συνδυασμό με τη διπλή κατεύθυνση είναι το πιο ισχυρό μοντέλο.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BE_m0jxgGRiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7:"
      ],
      "metadata": {
        "id": "A7K9dQLoJ4b8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjUQ2YlaJ5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8:"
      ],
      "metadata": {
        "id": "JpUofT3cJ6Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "na9QE4C4J7F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9:"
      ],
      "metadata": {
        "id": "ev7mwx9wJ7kD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "It4BARm4J8u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 10:"
      ],
      "metadata": {
        "id": "JhhLioCJJ9A1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07IZ7EGtJ-Ub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}