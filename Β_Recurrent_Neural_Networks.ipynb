{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUiI9WeNHAyF7kqZ7bkHRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrigoridou/Recurrent-Neural-Networks/blob/main/%CE%92_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Εγκατάσταση Απαιτούμενων Βιβλιοθηκών"
      ],
      "metadata": {
        "id": "GrIsLAs_v-RR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pxx07rpt-xd",
        "outputId": "832c6bd2-b581-4a82-be8f-36626ebc30dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.5.1\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Collecting torch==1.13.0 (from torchtext)\n",
            "  Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.12.14)\n",
            "Using cached torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torchtext\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecl3NueoNBcT",
        "outputId": "5904cc03-4084-4449-8d8c-5c11dff00ad8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "_umWPsLtoHd4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "hWSBZ_32oFtJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv\", \"r\") as file:\n",
        "    text = file.read()\n"
      ],
      "metadata": {
        "id": "TDgaukMONKj2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtPssc0vDSy",
        "outputId": "30ac40c8-ee31-400d-9cd3-2c1082679350"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update paths in the RNN code\n",
        "updated_rnn_code = rnn_code.replace('ag-news-classification-dataset/train.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
        "updated_rnn_code = updated_rnn_code.replace('ag-news-classification-dataset/test.csv', '/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
        "\n",
        "# Save the updated file to the correct location\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Verify the updated code\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Print the updated content to verify\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNNkvga5u9n7",
        "outputId": "a273193b-ab54-4d72-fcf7-435fd23702c9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Εκτέλεση του κώδικα"
      ],
      "metadata": {
        "id": "qADmQnx10zNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6790ad-7e24-4fc9-ee1d-bf72096c3ca0",
        "id": "-U2FoFbhufTk"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True)\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2136284\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:16<00:00,  6.88it/s]\n",
            "Train Loss : 1.298\n",
            "Valid Loss : 1.160 Valid Acc  : 0.577\n",
            "Epoch: 2\n",
            "100% 112/112 [00:15<00:00,  7.09it/s]\n",
            "Train Loss : 1.073\n",
            "Valid Loss : 1.017 Valid Acc  : 0.727\n",
            "Epoch: 3\n",
            "100% 112/112 [00:16<00:00,  6.96it/s]\n",
            "Train Loss : 0.981\n",
            "Valid Loss : 0.964 Valid Acc  : 0.781\n",
            "Epoch: 4\n",
            "100% 112/112 [00:16<00:00,  6.91it/s]\n",
            "Train Loss : 0.935\n",
            "Valid Loss : 0.934 Valid Acc  : 0.807\n",
            "Epoch: 5\n",
            "100% 112/112 [00:16<00:00,  6.85it/s]\n",
            "Train Loss : 0.910\n",
            "Valid Loss : 0.918 Valid Acc  : 0.825\n",
            "Epoch: 6\n",
            "100% 112/112 [00:16<00:00,  6.95it/s]\n",
            "Train Loss : 0.895\n",
            "Valid Loss : 0.913 Valid Acc  : 0.830\n",
            "Epoch: 7\n",
            "100% 112/112 [00:15<00:00,  7.03it/s]\n",
            "Train Loss : 0.882\n",
            "Valid Loss : 0.901 Valid Acc  : 0.842\n",
            "Epoch: 8\n",
            "100% 112/112 [00:16<00:00,  6.68it/s]\n",
            "Train Loss : 0.872\n",
            "Valid Loss : 0.897 Valid Acc  : 0.843\n",
            "Epoch: 9\n",
            "100% 112/112 [00:16<00:00,  6.70it/s]\n",
            "Train Loss : 0.864\n",
            "Valid Loss : 0.890 Valid Acc  : 0.851\n",
            "Epoch: 10\n",
            "100% 112/112 [00:16<00:00,  6.96it/s]\n",
            "Train Loss : 0.858\n",
            "Valid Loss : 0.886 Valid Acc  : 0.855\n",
            "Epoch: 11\n",
            "100% 112/112 [00:16<00:00,  6.97it/s]\n",
            "Train Loss : 0.851\n",
            "Valid Loss : 0.883 Valid Acc  : 0.859\n",
            "Epoch: 12\n",
            "100% 112/112 [00:16<00:00,  6.91it/s]\n",
            "Train Loss : 0.847\n",
            "Valid Loss : 0.881 Valid Acc  : 0.862\n",
            "Epoch: 13\n",
            "100% 112/112 [00:18<00:00,  5.90it/s]\n",
            "Train Loss : 0.842\n",
            "Valid Loss : 0.881 Valid Acc  : 0.862\n",
            "Epoch: 14\n",
            "100% 112/112 [00:19<00:00,  5.82it/s]\n",
            "Train Loss : 0.839\n",
            "Valid Loss : 0.879 Valid Acc  : 0.863\n",
            "Epoch: 15\n",
            "100% 112/112 [00:20<00:00,  5.56it/s]\n",
            "Train Loss : 0.838\n",
            "Valid Loss : 0.878 Valid Acc  : 0.864\n",
            "\n",
            "Test Accuracy : 0.861\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.88      0.85      0.86      1900\n",
            "      Sports       0.91      0.92      0.92      1900\n",
            "    Business       0.83      0.82      0.83      1900\n",
            "    Sci/Tech       0.82      0.85      0.83      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1614   92  135   59]\n",
            " [  64 1753   17   66]\n",
            " [  88   22 1561  229]\n",
            " [  68   53  165 1614]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια στο Test Set: 86.2%\n",
        "(ii) Ακρίβεια στο Validation Set: 86.6%\n",
        "(iii) Πλήθος Παραμέτρων: 2136284\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης ανά Εποχή: περίπου 16-17 δευτερόλεπτα ανά εποχή.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kDRQv80u3V0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Δι-κατευθυντήριο RNN"
      ],
      "metadata": {
        "id": "tWyIvMXA0vr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δι-κατευθυντήριο RNN: Στην τρέχουσα εκδοχή του κώδικα, το RNN layer είναι μονοκατευθυντικό (unidirectional). Για να το κάνουμε δι-κατευθυντήριο, πρέπει να προσθέσουμε το bidirectional=True στον ορισμό του RNN layer. Επίσης, επειδή το δι-κατευθυντήριο RNN συνενώνει τις εξόδους των δύο κατευθύνσεων, το μέγεθος του διανύσματος εξόδου θα διπλασιαστεί, οπότε θα πρέπει να αυξήσουμε το μέγεθος της εξόδου του γραμμικού (linear) layer για να το αντιστοιχίσουμε με το νέο μέγεθος του διανύσματος εξόδου."
      ],
      "metadata": {
        "id": "LlA5CZU74Eik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αλλαγές στον κώδικα:\n",
        "\n",
        "*   Τροποποιούμε την κλάση του RNN για να είναι δι-κατευθυντική.\n",
        "*   Διπλασιάζουμε το μέγεθος των εξόδων του RNN στον γραμμικό (linear) layer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3XpBfBFf4cFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Διαβάζουμε το περιεχόμενο του αρχείου rnn.py\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εμφανίζουμε το περιεχόμενο του αρχείου για να το δούμε\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "HzgBl1Zr09RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afc8954-e1de-4094-a9d8-31961c4b66cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
            "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικαθιστούμε το RNN με bidirectional RNN και προσαρμόζουμε το γραμμικό layer\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'  # Για διπλασιασμένο hidden_dim λόγω bidirectional\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "KgWU52HE1A6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f553aa-2b28-4caa-c459-a05d169d9e67"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXpDw-Hu6isI",
        "outputId": "41193578-569f-4823-d19f-06165cd4f376"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2147164\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "100% 112/112 [00:27<00:00,  4.12it/s]\n",
            "Train Loss : 1.308\n",
            "Valid Loss : 1.159 Valid Acc  : 0.593\n",
            "Epoch: 2\n",
            "100% 112/112 [00:28<00:00,  3.96it/s]\n",
            "Train Loss : 1.075\n",
            "Valid Loss : 1.038 Valid Acc  : 0.706\n",
            "Epoch: 3\n",
            "100% 112/112 [00:26<00:00,  4.19it/s]\n",
            "Train Loss : 0.986\n",
            "Valid Loss : 0.978 Valid Acc  : 0.764\n",
            "Epoch: 4\n",
            "100% 112/112 [00:23<00:00,  4.83it/s]\n",
            "Train Loss : 0.942\n",
            "Valid Loss : 0.959 Valid Acc  : 0.783\n",
            "Epoch: 5\n",
            "100% 112/112 [00:24<00:00,  4.61it/s]\n",
            "Train Loss : 0.917\n",
            "Valid Loss : 0.939 Valid Acc  : 0.803\n",
            "Epoch: 6\n",
            "100% 112/112 [00:33<00:00,  3.39it/s]\n",
            "Train Loss : 0.899\n",
            "Valid Loss : 0.921 Valid Acc  : 0.820\n",
            "Epoch: 7\n",
            "100% 112/112 [00:23<00:00,  4.84it/s]\n",
            "Train Loss : 0.885\n",
            "Valid Loss : 0.916 Valid Acc  : 0.824\n",
            "Epoch: 8\n",
            "100% 112/112 [00:24<00:00,  4.54it/s]\n",
            "Train Loss : 0.876\n",
            "Valid Loss : 0.908 Valid Acc  : 0.835\n",
            "Epoch: 9\n",
            "100% 112/112 [00:24<00:00,  4.67it/s]\n",
            "Train Loss : 0.868\n",
            "Valid Loss : 0.909 Valid Acc  : 0.832\n",
            "Epoch: 10\n",
            "100% 112/112 [00:23<00:00,  4.86it/s]\n",
            "Train Loss : 0.862\n",
            "Valid Loss : 0.899 Valid Acc  : 0.845\n",
            "Epoch: 11\n",
            "100% 112/112 [00:24<00:00,  4.59it/s]\n",
            "Train Loss : 0.857\n",
            "Valid Loss : 0.896 Valid Acc  : 0.847\n",
            "Epoch: 12\n",
            "100% 112/112 [00:23<00:00,  4.69it/s]\n",
            "Train Loss : 0.850\n",
            "Valid Loss : 0.892 Valid Acc  : 0.852\n",
            "Epoch: 13\n",
            "100% 112/112 [00:23<00:00,  4.75it/s]\n",
            "Train Loss : 0.846\n",
            "Valid Loss : 0.890 Valid Acc  : 0.852\n",
            "Epoch: 14\n",
            "100% 112/112 [00:24<00:00,  4.50it/s]\n",
            "Train Loss : 0.844\n",
            "Valid Loss : 0.888 Valid Acc  : 0.853\n",
            "Epoch: 15\n",
            "100% 112/112 [00:24<00:00,  4.64it/s]\n",
            "Train Loss : 0.842\n",
            "Valid Loss : 0.887 Valid Acc  : 0.855\n",
            "\n",
            "Test Accuracy : 0.861\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.88      0.86      0.87      1900\n",
            "      Sports       0.92      0.92      0.92      1900\n",
            "    Business       0.83      0.81      0.82      1900\n",
            "    Sci/Tech       0.81      0.86      0.84      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1633   82  120   65]\n",
            " [  55 1742   19   84]\n",
            " [ 113   25 1532  230]\n",
            " [  52   36  175 1637]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(i) Ακρίβεια Ταξινόμησης στο Test Set: 86.3%\n",
        "(ii) Μέση Ακρίβεια Ταξινόμησης στο Validation Set: 84.2%\n",
        "(iii) Πλήθος Παραμέτρων Μοντέλου: 2147164\n",
        "(iv) Μέσο Χρονικό Κόστος Εκπαίδευσης Ανά Εποχή: 25 δευτερόλεπτα\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8a_ewFOF8uug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Διπλό στρώμα δι-κατευθυντήριου RNN"
      ],
      "metadata": {
        "id": "6U8igk7r1C36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Αλλαγή στην αρχικοποίηση του RNN:\n",
        "\n",
        "\n",
        "*   Αυξάνουμε τον αριθμό των επιπέδων του RNN από 1 σε 2. Αυτό γίνεται με την παράμετρο num_layers=2.\n",
        "*   Ρυθμίζουμε το RNN ώστε να είναι δι-κατευθυντήριο με την παράμετρο bidirectional=True.\n",
        "\n",
        "\n",
        "\n",
        "##2. Αλλαγή στον γραμμικό (linear) layer:\n",
        "\n",
        "Επειδή το δι-κατευθυντήριο RNN διπλασιάζει το μέγεθος του hidden state, και\n",
        "\n",
        "*   τώρα έχουμε δύο στρώματα, το μέγεθος του hidden state θα είναι 2 * hidden_dim (διπλό λόγω δι-κατευθυντηριότητας και λόγω δύο στρωμάτων). Άρα το hidden_dim πρέπει να πολλαπλασιαστεί με 2 ακόμα, δηλαδή το νέο μέγεθος του hidden state είναι hidden_dim * 2 * 2 λόγω δύο επιπέδων και δι-κατευθυντηριότητας.\n",
        "\n"
      ],
      "metadata": {
        "id": "xdpR_QNQ_Wpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημερώνουμε το RNN για να είναι δύο στρώματα δι-κατευθυντήριου\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημερώνουμε το γραμμικό layer για να έχει το σωστό μέγεθος εξόδου\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)'  # Για δύο στρώματα δι-κατευθυντήριου RNN, το μέγεθος είναι hidden_dim * 2 * 2\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "VuQorBTt1EL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f63aad-a2ba-42e9-89f4-159bba2dff35"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvrZXvRe_PAY",
        "outputId": "a7d69fe6-f115-4aa4-9938-0da53f20ba89"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2172508\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "  0% 0/112 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 166, in <module>\n",
            "    TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 152, in TrainModel\n",
            "    Y_preds = model(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 99, in forward\n",
            "    logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x128 and 256x4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Χρήση LSTM"
      ],
      "metadata": {
        "id": "B4VaUn8_1Fm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 1: Αντικατάσταση του RNN με LSTM"
      ],
      "metadata": {
        "id": "zs4KIavuFXHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα αντικαταστήσουμε την αρχική χρήση του RNN με LSTM και θα κάνουμε τις απαραίτητες προσαρμογές στον κώδικα για να δουλέψει σωστά με το LSTM. Παρακάτω παρατίθεται το κομμάτι κώδικα που πρέπει να τροποποιηθεί για να χρησιμοποιηθεί LSTM αντί για RNN.\n",
        "\n",
        "Αλλαγές στον κώδικα:\n",
        "\n",
        "*   Αντικαθιστούμε το nn.RNN με nn.LSTM.\n",
        "*   Επειδή το LSTM έχει δύο εξόδους, h_n και c_n, πρέπει να προσαρμόσουμε την εξαγωγή της εξόδου για να χρησιμοποιήσουμε μόνο την πρώτη έξοδο (που είναι το h_n).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bB_Kgnz8Fcf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικατάσταση του RNN με LSTM\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "id": "qLl4K2Lh1G6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2aa158c-5475-45e3-b661-93d803491ca7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R74ACC5GLyI",
        "outputId": "ff2be8d5-0173-41d4-db45-23a570f0fd6e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2172508\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "  0% 0/112 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 166, in <module>\n",
            "    TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 152, in TrainModel\n",
            "    Y_preds = model(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 99, in forward\n",
            "    logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x128 and 256x4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 2: Ενημέρωση για Δι-κατευθυντήριο LSTM"
      ],
      "metadata": {
        "id": "GdQAShAXFrIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Όπως κάναμε για το RNN, θα προσθέσουμε την παράμετρο bidirectional=True για να το κάνουμε δι-κατευθυντήριο. Αυτό θα διπλασιάσει το μέγεθος του hidden state."
      ],
      "metadata": {
        "id": "e9XLop2gFvnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημέρωση για Δι-κατευθυντήριο LSTM\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημέρωση του γραμμικού layer για διπλασιασμένο μέγεθος εξόδου\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)'\n",
        ")\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9LKpleuFw8u",
        "outputId": "ebd84fb0-090f-4d97-b6d5-80be51295e8c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mowdBysYGKXW",
        "outputId": "834fe0f5-644d-4b0e-ce91-50134fbf33ba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2172508\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "  0% 0/112 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 166, in <module>\n",
            "    TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 152, in TrainModel\n",
            "    Y_preds = model(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 99, in forward\n",
            "    logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x128 and 256x4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Βήμα 3: Ενημέρωση για Διπλό Στρώμα Δι-κατευθυντήριου LSTM"
      ],
      "metadata": {
        "id": "LOW-c7bxFz_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Όπως και με το RNN, θα προσθέσουμε και ένα δεύτερο επίπεδο στο LSTM και θα διπλασιάσουμε το μέγεθος του hidden state."
      ],
      "metadata": {
        "id": "q_cM4W_NF2g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ενημέρωση για Διπλό Στρώμα Δι-κατευθυντήριο LSTM\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)',\n",
        "    'self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)'\n",
        ")\n",
        "\n",
        "# Ενημέρωση του γραμμικού layer για διπλό στρώμα\n",
        "updated_rnn_code = updated_rnn_code.replace(\n",
        "    'self.linear = nn.Linear(hidden_dim * 2, output_dim)',\n",
        "    'self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)'  # Για δύο στρώματα LSTM, το μέγεθος είναι hidden_dim * 2 * 2\n",
        ")\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8H24ZfNFsHw",
        "outputId": "78983407-face-4450-e313-a8b5d48b602d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "\n",
            "A RNN classifier applied to AG_NEWS dataset\n",
            "\n",
            "Download dataset:\n",
            "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "from torchtext.data import get_tokenizer\n",
            "from torchtext.vocab import build_vocab_from_iterator\n",
            "from torch.utils.data.dataset import random_split\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# HYPER-PARAMETERS\n",
            "MAX_WORDS = 25\n",
            "EPOCHS = 15\n",
            "LEARNING_RATE = 1e-3\n",
            "BATCH_SIZE = 1024\n",
            "EMBEDDING_DIM = 100\n",
            "HIDDEN_DIM = 64\n",
            "\n",
            "######################################################################\n",
            "# Read dataset files \n",
            "# ------------------\n",
            "\n",
            "\n",
            "train_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/train.csv')\n",
            "test_data = pd.read_csv('/content/drive/MyDrive/RecurrentNeuralNetworks/ag-news-classification-dataset/test.csv')\n",
            "\n",
            "######################################################################\n",
            "# Data splitting and processing \n",
            "# -----------------------------\n",
            "\n",
            "\n",
            "tokenizer = get_tokenizer(\"basic_english\")\n",
            "\n",
            "# All texts are truncated and padded to MAX_WORDS tokens\n",
            "def collate_batch(batch):\n",
            "    Y, X = list(zip(*batch))\n",
            "    Y = torch.tensor(Y) - 1 # Target names in range [0,1,2,3] instead of [1,2,3,4]\n",
            "    X = [vocab(tokenizer(text)) for text in X]\n",
            "    # Bringing all samples to MAX_WORDS length. Shorter texts are padded with <PAD> sequences, longer texts are truncated.\n",
            "    X = [tokens+([vocab['<PAD>']]* (MAX_WORDS-len(tokens))) if len(tokens)<MAX_WORDS else tokens[:MAX_WORDS] for tokens in X]\n",
            "    return torch.tensor(X, dtype=torch.int32).to(device), Y.to(device) \n",
            "\n",
            "train_dataset = [(label,train_data['Title'][i] + ' ' + train_data['Description'][i]) for i,label in enumerate(train_data['Class Index'])]\n",
            "test_dataset = [(label,test_data['Title'][i] + ' ' + test_data['Description'][i]) for i,label in enumerate(test_data['Class Index'])]\n",
            "\n",
            "# Validation set is a randomly-selected 5% of the initial training set\n",
            "num_train = int(len(train_dataset) * 0.95)\n",
            "split_train_, split_valid_ = \\\n",
            "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
            "\n",
            "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=True, collate_fn=collate_batch)\n",
            "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
            "                              shuffle=False, collate_fn=collate_batch)\n",
            "\n",
            "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
            "\n",
            "def build_vocabulary(datasets):\n",
            "    for dataset in datasets:\n",
            "        for _, text in dataset:\n",
            "            yield tokenizer(text)\n",
            "\n",
            "# Vocabulary includes all tokens with at least 10 occurrences in the training texts\n",
            "# Special tokens <PAD> and <UNK> are used for padding sequences and unknown words respectively\n",
            "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=10, specials=[\"<PAD>\",\"<UNK>\"])\n",
            "vocab.set_default_index(vocab[\"<UNK>\"])\n",
            "\n",
            "######################################################################\n",
            "# Define the model\n",
            "# ----------------\n",
            "\n",
            "\n",
            "class model(nn.Module):\n",
            "    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n",
            "        super(model, self).__init__()\n",
            "        self.embedding_layer = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
            "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        self.linear = nn.Linear(hidden_dim * 2 * 2, output_dim)\n",
            "\n",
            "    def forward(self, X_batch):\n",
            "        embeddings = self.embedding_layer(X_batch)\n",
            "        output, hidden = self.rnn(embeddings)\n",
            "        logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "        probs = F.softmax(logits, dim=1)\n",
            "        return probs\n",
            "    \n",
            "######################################################################\n",
            "# Initiate an instance of the model\n",
            "# ---------------------------------\n",
            "\n",
            "\n",
            "classifier = model(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, len(target_classes)).to(device)\n",
            "# Define loss function and opimization algorithm\n",
            "loss_fn = nn.CrossEntropyLoss()\n",
            "optimizer = torch.optim.Adam([param for param in classifier.parameters() if param.requires_grad == True],lr=LEARNING_RATE)\n",
            "\n",
            "# Count model parameters\n",
            "def count_parameters(model):\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "print('\\nModel:')\n",
            "print(classifier)\n",
            "print('Total parameters: ',count_parameters(classifier))\n",
            "print('\\n\\n')\n",
            "\n",
            "######################################################################\n",
            "# Define functions to train and evaluate the model\n",
            "# ------------------------------------------------\n",
            "\n",
            "\n",
            "def EvaluateModel(model, loss_fn, val_loader):\n",
            "    model.eval()\n",
            "    with torch.no_grad():\n",
            "        Y_actual, Y_preds, losses = [],[],[]\n",
            "        for X, Y in val_loader:\n",
            "            preds = model(X)\n",
            "            loss = loss_fn(preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            Y_actual.append(Y)\n",
            "            Y_preds.append(preds.argmax(dim=-1))\n",
            "\n",
            "        Y_actual = torch.cat(Y_actual)\n",
            "        Y_preds = torch.cat(Y_preds)\n",
            "    \n",
            "    # Returns mean loss, actual labels, predicted labels \n",
            "    return torch.tensor(losses).mean(), Y_actual.detach().cpu().numpy(), Y_preds.detach().cpu().numpy()\n",
            "\n",
            "\n",
            "def TrainModel(model, loss_fn, optimizer, train_loader, valid_loader, epochs):\n",
            "    for i in range(1, epochs+1):\n",
            "        model.train()\n",
            "        print('Epoch:',i)\n",
            "        losses = []\n",
            "        for X, Y in tqdm(train_loader):\n",
            "            Y_preds = model(X)\n",
            "\n",
            "            loss = loss_fn(Y_preds, Y)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "            optimizer.zero_grad()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "\n",
            "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
            "        valid_loss, valid_actual, valid_preds = EvaluateModel(model, loss_fn, valid_loader)\n",
            "        print(\"Valid Loss : {:.3f}\".format(valid_loss),\"Valid Acc  : {:.3f}\".format(accuracy_score(valid_actual, valid_preds)))\n",
            "        \n",
            "\n",
            "TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "\n",
            "######################################################################\n",
            "# Evaluate the model with test dataset\n",
            "# ------------------------------------\n",
            "\n",
            "\n",
            "_, Y_actual, Y_preds = EvaluateModel(classifier, loss_fn, test_loader)\n",
            "\n",
            "print(\"\\nTest Accuracy : {:.3f}\".format(accuracy_score(Y_actual, Y_preds)))\n",
            "print(\"\\nClassification Report : \")\n",
            "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
            "print(\"\\nConfusion Matrix : \")\n",
            "print(confusion_matrix(Y_actual, Y_preds))\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Εκτέλεση του τροποποιημένου κώδικα\n",
        "!python3 /content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuT-JjXsGH6h",
        "outputId": "cc937e7a-9172-4c19-9aa8-e4572612da73"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:\n",
            "model(\n",
            "  (embedding_layer): Embedding(21254, 100)\n",
            "  (rnn): RNN(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "Total parameters:  2172508\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 1\n",
            "\r  0% 0/112 [00:00<?, ?it/s]\r  0% 0/112 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 166, in <module>\n",
            "    TrainModel(classifier, loss_fn, optimizer, train_loader, valid_loader, EPOCHS)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 152, in TrainModel\n",
            "    Y_preds = model(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py\", line 99, in forward\n",
            "    logits = self.linear(output[:,-1])  # The last output of RNN is used for sequence classification\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x128 and 256x4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pylPFPuGHvO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5:"
      ],
      "metadata": {
        "id": "gX3LsAFGJwgw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_QcmW5SJ1fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6:"
      ],
      "metadata": {
        "id": "N7YZYi7EJ2TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Αντικατάσταση της παραμέτρου MAX_WORDS\n",
        "updated_rnn_code = rnn_code.replace(\n",
        "    'MAX_WORDS = 25',  # Παλιό όριο λέξεων\n",
        "    'MAX_WORDS = 50'   # Νέο όριο λέξεων\n",
        ")\n",
        "\n",
        "\n",
        "# Αποθήκευση του τροποποιημένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'w') as file:\n",
        "    file.write(updated_rnn_code)\n",
        "\n",
        "# Επαλήθευση του ενημερωμένου κώδικα\n",
        "with open('/content/drive/MyDrive/RecurrentNeuralNetworks/rnn.py', 'r') as file:\n",
        "    rnn_code = file.read()\n",
        "\n",
        "# Εκτύπωση του ενημερωμένου κώδικα για επαλήθευση\n",
        "print(rnn_code)\n"
      ],
      "metadata": {
        "id": "uX9zUT_kJ305"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7:"
      ],
      "metadata": {
        "id": "A7K9dQLoJ4b8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjUQ2YlaJ5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8:"
      ],
      "metadata": {
        "id": "JpUofT3cJ6Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "na9QE4C4J7F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9:"
      ],
      "metadata": {
        "id": "ev7mwx9wJ7kD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "It4BARm4J8u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 10:"
      ],
      "metadata": {
        "id": "JhhLioCJJ9A1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07IZ7EGtJ-Ub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}